{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6911eaa6668b42d88cb3772c7d6c2b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f4eb657dad341ff8530754b05835a0f",
              "IPY_MODEL_a5a11dfb6cd8407ca2ce37fe9aa24218",
              "IPY_MODEL_fa0a535ac9014db6a5aaa38c2d143696"
            ],
            "layout": "IPY_MODEL_0ed0a0648d8746e5ba25067a44042b16"
          }
        },
        "3f4eb657dad341ff8530754b05835a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_455276130bd6492d9d77a470a19994c0",
            "placeholder": "​",
            "style": "IPY_MODEL_13aba50f3ac14ad5a1ed6212c1b03a96",
            "value": "100%"
          }
        },
        "a5a11dfb6cd8407ca2ce37fe9aa24218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a497ce77ab634c768e5a5e075416005b",
            "max": 30000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8429d769e71d4560ad1599c132a305a2",
            "value": 30000
          }
        },
        "fa0a535ac9014db6a5aaa38c2d143696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b280b1b2b745f697f5b576a82d7900",
            "placeholder": "​",
            "style": "IPY_MODEL_3371e3ebe6f7452dbf9501c898ac3489",
            "value": " 30000/30000 [03:28&lt;00:00, 155.38it/s]"
          }
        },
        "0ed0a0648d8746e5ba25067a44042b16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455276130bd6492d9d77a470a19994c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13aba50f3ac14ad5a1ed6212c1b03a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a497ce77ab634c768e5a5e075416005b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8429d769e71d4560ad1599c132a305a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39b280b1b2b745f697f5b576a82d7900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3371e3ebe6f7452dbf9501c898ac3489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTQQSIwS7Rxj",
        "outputId": "c0bb3751-c5d3-4b3d-9b13-ffbb83bb11e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install transformer_lens\n",
        "%pip -q install circuitsvis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "DJQgYixdN_ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import einops\n",
        "#from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "BDL7FPZB8ewF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "wGkh0Co48hLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ],
      "metadata": {
        "id": "Q-RYNUow8j_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/grokking_xor1.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ],
      "metadata": {
        "id": "oQIZVbPC8kl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET"
      ],
      "metadata": {
        "id": "c0c1EnmfOHga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def generate_xor_dataset(bit_length=1, n_samples=None, device='cpu', seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "    if n_samples is None:\n",
        "        all_pairs = list(product([0, 1], repeat=2 * bit_length))\n",
        "        #random.shuffle(all_pairs)\n",
        "        inputs = torch.tensor(all_pairs, dtype=torch.long, device=device)\n",
        "    else:\n",
        "        inputs = torch.randint(\n",
        "            0, 2, size=(n_samples, 2 * bit_length), dtype=torch.long, device=device\n",
        "        )\n",
        "\n",
        "    a = inputs[:, :bit_length]\n",
        "    b = inputs[:, bit_length:]\n",
        "    targets = torch.bitwise_xor(a, b)\n",
        "\n",
        "    return inputs, targets\n",
        "seed = 598\n",
        "dataset, labels = generate_xor_dataset(bit_length=4, device=device, seed=seed)"
      ],
      "metadata": {
        "id": "UAJh9wk08msm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "print(labels.shape)\n",
        "print(dataset)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkrGqZOJ6SBO",
        "outputId": "26bc1404-c57a-4ec5-f87d-cb42a8d38757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 8])\n",
            "torch.Size([256, 4])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 1],\n",
            "        [0, 0, 0,  ..., 0, 1, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 0, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')\n",
            "tensor([[0, 0, 0, 0],\n",
            "        [0, 0, 0, 1],\n",
            "        [0, 0, 1, 0],\n",
            "        ...,\n",
            "        [0, 0, 1, 0],\n",
            "        [0, 0, 0, 1],\n",
            "        [0, 0, 0, 0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "oGjKQ_YV7y_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frac_train = 0.115\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "num_epochs = 30000\n",
        "checkpoint_every = 100\n",
        "n_samples = 256\n",
        "bit_length = 4"
      ],
      "metadata": {
        "id": "CZ-QN6wZ7g5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "indices = torch.randperm(n_samples)\n",
        "cutoff = int(n_samples*frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]\n",
        "print(train_data[:5])\n",
        "print(train_labels[:5])\n",
        "print(train_data.shape)\n",
        "print(test_data[:5])\n",
        "print(test_labels[:5])\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWFsWaLk75U6",
        "outputId": "57045bc9-98ab-47e3-c5d2-69303e4b7f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 1, 1, 1, 1],\n",
            "        [0, 0, 1, 0, 0, 1, 1, 0],\n",
            "        [1, 0, 0, 1, 1, 1, 0, 1],\n",
            "        [1, 1, 1, 0, 0, 1, 0, 1],\n",
            "        [0, 1, 1, 0, 1, 1, 1, 0]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 1],\n",
            "        [0, 1, 0, 0],\n",
            "        [0, 1, 0, 0],\n",
            "        [1, 0, 1, 1],\n",
            "        [1, 0, 0, 0]], device='cuda:0')\n",
            "torch.Size([29, 8])\n",
            "tensor([[0, 1, 0, 1, 1, 0, 1, 1],\n",
            "        [0, 0, 0, 1, 1, 0, 0, 1],\n",
            "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
            "        [0, 0, 1, 1, 0, 1, 0, 1],\n",
            "        [0, 1, 1, 1, 0, 0, 1, 1]], device='cuda:0')\n",
            "tensor([[1, 1, 1, 0],\n",
            "        [1, 0, 0, 0],\n",
            "        [1, 1, 1, 0],\n",
            "        [0, 1, 1, 0],\n",
            "        [0, 1, 0, 0]], device='cuda:0')\n",
            "torch.Size([227, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "9TzmaSZF_JJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 2,\n",
        "    d_model = 64,\n",
        "    d_head = 32,\n",
        "    d_mlp = 256,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=2,\n",
        "    d_vocab_out=2,\n",
        "    n_ctx=8,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")"
      ],
      "metadata": {
        "id": "7-KxBipj-bdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer(cfg)"
      ],
      "metadata": {
        "id": "NJnFgeBu_TCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable the biases, as we don't need them for this task and it makes things easier to interpret.\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "DDO492qp_Vt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ],
      "metadata": {
        "id": "fY9zsBPd_YFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(train_data)\n",
        "print(logits.shape)\n",
        "log_probs = logits[:,-1,:].log_softmax(dim=-1)\n",
        "print(log_probs.shape)\n",
        "print(train_labels.shape)\n",
        "print(train_labels.unsqueeze(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhU_7mYaLZPL",
        "outputId": "7fc30a6d-98c4-4342-d1e9-e458ebc29f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([29, 8, 2])\n",
            "torch.Size([29, 2])\n",
            "torch.Size([29, 4])\n",
            "torch.Size([29, 4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "      logits = logits[:,4:8,:]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1MeYPU6_alq",
        "outputId": "fe991012-febd-416d-8200-4347303c7776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7264, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(0.7047, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TfLH5AvVOWYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "    train_logits = model(train_data)\n",
        "    train_loss = loss_fn(train_logits, train_labels)\n",
        "    train_loss.backward()\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        test_logits = model(test_data)\n",
        "        test_loss = loss_fn(test_logits, test_labels)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "    if ((epoch+1)%checkpoint_every)==0:\n",
        "        checkpoint_epochs.append(epoch)\n",
        "        model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "        print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6911eaa6668b42d88cb3772c7d6c2b95",
            "3f4eb657dad341ff8530754b05835a0f",
            "a5a11dfb6cd8407ca2ce37fe9aa24218",
            "fa0a535ac9014db6a5aaa38c2d143696",
            "0ed0a0648d8746e5ba25067a44042b16",
            "455276130bd6492d9d77a470a19994c0",
            "13aba50f3ac14ad5a1ed6212c1b03a96",
            "a497ce77ab634c768e5a5e075416005b",
            "8429d769e71d4560ad1599c132a305a2",
            "39b280b1b2b745f697f5b576a82d7900",
            "3371e3ebe6f7452dbf9501c898ac3489"
          ]
        },
        "id": "39f8S-ry_dtO",
        "outputId": "1be16b49-ae81-473e-c26b-03f27e3e016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6911eaa6668b42d88cb3772c7d6c2b95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99 Train Loss 0.030021441755825495 Test Loss 0.8303669517703793\n",
            "Epoch 199 Train Loss 0.00037066741755052544 Test Loss 1.1042912855176592\n",
            "Epoch 299 Train Loss 0.00012115187137547482 Test Loss 1.2339163216111297\n",
            "Epoch 399 Train Loss 3.830071327049715e-05 Test Loss 1.3786713926666785\n",
            "Epoch 499 Train Loss 1.2400633690129767e-05 Test Loss 1.525739116083395\n",
            "Epoch 599 Train Loss 4.011186973046374e-06 Test Loss 1.6780897942100317\n",
            "Epoch 699 Train Loss 1.327954423996522e-06 Test Loss 1.8298483338922962\n",
            "Epoch 799 Train Loss 4.7346538585037905e-07 Test Loss 1.9673466589305477\n",
            "Epoch 899 Train Loss 1.7483924014203614e-07 Test Loss 2.080839493487997\n",
            "Epoch 999 Train Loss 7.025665648776926e-08 Test Loss 2.1653758422823217\n",
            "Epoch 1099 Train Loss 3.1239919446831464e-08 Test Loss 2.2037760632513987\n",
            "Epoch 1199 Train Loss 1.5496999282827136e-08 Test Loss 2.1918121112687063\n",
            "Epoch 1299 Train Loss 8.45381657375591e-09 Test Loss 2.137310156716523\n",
            "Epoch 1399 Train Loss 5.69193332523595e-09 Test Loss 2.043283350225211\n",
            "Epoch 1499 Train Loss 4.7684244000352715e-09 Test Loss 1.9301577735220787\n",
            "Epoch 1599 Train Loss 4.747013667426136e-09 Test Loss 1.81656456079746\n",
            "Epoch 1699 Train Loss 5.010277267704818e-09 Test Loss 1.7169671341452157\n",
            "Epoch 1799 Train Loss 5.289006913350734e-09 Test Loss 1.6342768903037506\n",
            "Epoch 1899 Train Loss 5.505285761842014e-09 Test Loss 1.569723658242157\n",
            "Epoch 1999 Train Loss 5.680443653348056e-09 Test Loss 1.5178056864533713\n",
            "Epoch 2099 Train Loss 5.787054557154552e-09 Test Loss 1.473176131359767\n",
            "Epoch 2199 Train Loss 5.580579233540963e-09 Test Loss 1.383528763745615\n",
            "Epoch 2299 Train Loss 5.6647822414140595e-09 Test Loss 1.3012494185588033\n",
            "Epoch 2399 Train Loss 5.735916146243788e-09 Test Loss 1.246627768327684\n",
            "Epoch 2499 Train Loss 5.817398023929484e-09 Test Loss 1.210816177273035\n",
            "Epoch 2599 Train Loss 5.842837448875596e-09 Test Loss 1.1820988767422198\n",
            "Epoch 2699 Train Loss 5.8636144039892124e-09 Test Loss 1.158206330216195\n",
            "Epoch 2799 Train Loss 5.867239456516433e-09 Test Loss 1.1438923098238758\n",
            "Epoch 2899 Train Loss 5.87442149523327e-09 Test Loss 1.12748453406171\n",
            "Epoch 2999 Train Loss 5.8466283505973625e-09 Test Loss 1.1122193268370844\n",
            "Epoch 3099 Train Loss 5.840888762484535e-09 Test Loss 1.0959707150205706\n",
            "Epoch 3199 Train Loss 5.80649025836895e-09 Test Loss 1.0813942191256698\n",
            "Epoch 3299 Train Loss 5.7808880682735584e-09 Test Loss 1.0595528656530309\n",
            "Epoch 3399 Train Loss 5.776931080130439e-09 Test Loss 1.0383280162284634\n",
            "Epoch 3499 Train Loss 5.739692931747796e-09 Test Loss 1.0150938664198663\n",
            "Epoch 3599 Train Loss 5.720264302794772e-09 Test Loss 0.9957028497948774\n",
            "Epoch 3699 Train Loss 5.6928570106292295e-09 Test Loss 0.9738472551756212\n",
            "Epoch 3799 Train Loss 5.687345616654687e-09 Test Loss 0.9547813751890785\n",
            "Epoch 3899 Train Loss 5.654499372859775e-09 Test Loss 0.9309519901717714\n",
            "Epoch 3999 Train Loss 5.644877232641855e-09 Test Loss 0.909844954644582\n",
            "Epoch 4099 Train Loss 5.610101997485866e-09 Test Loss 0.8865819951162716\n",
            "Epoch 4199 Train Loss 5.582740570827887e-09 Test Loss 0.8650974660506023\n",
            "Epoch 4299 Train Loss 5.585309833155858e-09 Test Loss 0.8417833124102813\n",
            "Epoch 4399 Train Loss 5.563009327477128e-09 Test Loss 0.8212883157004272\n",
            "Epoch 4499 Train Loss 5.5509863646077906e-09 Test Loss 0.7993016323633122\n",
            "Epoch 4599 Train Loss 5.535708170610993e-09 Test Loss 0.780865140254058\n",
            "Epoch 4699 Train Loss 5.520524937150964e-09 Test Loss 0.7610541625049964\n",
            "Epoch 4799 Train Loss 5.5032329975833475e-09 Test Loss 0.7453460598701513\n",
            "Epoch 4899 Train Loss 5.502489544455504e-09 Test Loss 0.7277134487138913\n",
            "Epoch 4999 Train Loss 5.499194670054197e-09 Test Loss 0.7127430920480617\n",
            "Epoch 5099 Train Loss 5.4670790487097585e-09 Test Loss 0.6969273012453467\n",
            "Epoch 5199 Train Loss 5.44828679203339e-09 Test Loss 0.6820935312392591\n",
            "Epoch 5299 Train Loss 5.44520823522223e-09 Test Loss 0.6666032847320691\n",
            "Epoch 5399 Train Loss 5.43132813720522e-09 Test Loss 0.6525343045028221\n",
            "Epoch 5499 Train Loss 5.4088373925817575e-09 Test Loss 0.6408854612346889\n",
            "Epoch 5599 Train Loss 5.388206115820704e-09 Test Loss 0.6299279043648137\n",
            "Epoch 5699 Train Loss 5.400351373548606e-09 Test Loss 0.6196366988195168\n",
            "Epoch 5799 Train Loss 5.3907695075381054e-09 Test Loss 0.6092062035446348\n",
            "Epoch 5899 Train Loss 5.382428612442071e-09 Test Loss 0.5996057268734192\n",
            "Epoch 5999 Train Loss 5.379483058233411e-09 Test Loss 0.5904407817164469\n",
            "Epoch 6099 Train Loss 5.390505502730291e-09 Test Loss 0.581308401013421\n",
            "Epoch 6199 Train Loss 5.349570522502222e-09 Test Loss 0.57055206612319\n",
            "Epoch 6299 Train Loss 5.356633287281113e-09 Test Loss 0.56097272671665\n",
            "Epoch 6399 Train Loss 5.34966640060063e-09 Test Loss 0.5524096374025228\n",
            "Epoch 6499 Train Loss 5.338311458972612e-09 Test Loss 0.5429039197475501\n",
            "Epoch 6599 Train Loss 5.321547414903294e-09 Test Loss 0.5333400190225343\n",
            "Epoch 6699 Train Loss 4.831277037103154e-09 Test Loss 0.5091636077298006\n",
            "Epoch 6799 Train Loss 5.029488306957509e-09 Test Loss 0.4708661549592013\n",
            "Epoch 6899 Train Loss 5.026927578622881e-09 Test Loss 0.4381312865856115\n",
            "Epoch 6999 Train Loss 5.029650782312762e-09 Test Loss 0.41265424436097264\n",
            "Epoch 7099 Train Loss 5.025078058296446e-09 Test Loss 0.3913216034059306\n",
            "Epoch 7199 Train Loss 5.026371528239141e-09 Test Loss 0.3721204926351055\n",
            "Epoch 7299 Train Loss 5.043798160097883e-09 Test Loss 0.3564071618005834\n",
            "Epoch 7399 Train Loss 5.022973717803922e-09 Test Loss 0.34093891296470047\n",
            "Epoch 7499 Train Loss 5.036062862429603e-09 Test Loss 0.32723946036294205\n",
            "Epoch 7599 Train Loss 5.03983997626193e-09 Test Loss 0.3149666464632565\n",
            "Epoch 7699 Train Loss 5.037863243445581e-09 Test Loss 0.30120751671512647\n",
            "Epoch 7799 Train Loss 5.034197374215422e-09 Test Loss 0.2888294966042517\n",
            "Epoch 7899 Train Loss 5.0230901612573064e-09 Test Loss 0.2777140863109493\n",
            "Epoch 7999 Train Loss 5.024285720765402e-09 Test Loss 0.2654400514154678\n",
            "Epoch 8099 Train Loss 5.0031174653359885e-09 Test Loss 0.2525205511151959\n",
            "Epoch 8199 Train Loss 4.348969201595602e-09 Test Loss 0.21833564884175052\n",
            "Epoch 8299 Train Loss 4.310603624926805e-09 Test Loss 0.1553567627523187\n",
            "Epoch 8399 Train Loss 4.531048580364506e-09 Test Loss 0.1177059905605613\n",
            "Epoch 8499 Train Loss 4.577441693977599e-09 Test Loss 0.09052598013497905\n",
            "Epoch 8599 Train Loss 4.546190006389494e-09 Test Loss 0.0728442095818049\n",
            "Epoch 8699 Train Loss 3.6069427162947512e-09 Test Loss 0.05197018401461376\n",
            "Epoch 8799 Train Loss 3.792033368852205e-09 Test Loss 0.02888897467718832\n",
            "Epoch 8899 Train Loss 3.8053001788515475e-09 Test Loss 0.014632259029005622\n",
            "Epoch 8999 Train Loss 3.849080321461006e-09 Test Loss 0.0059654137670232844\n",
            "Epoch 9099 Train Loss 3.931829409123038e-09 Test Loss 0.002118298490226002\n",
            "Epoch 9199 Train Loss 3.988300179661155e-09 Test Loss 0.0009897383945753249\n",
            "Epoch 9299 Train Loss 4.054362029411225e-09 Test Loss 0.0007281135685741908\n",
            "Epoch 9399 Train Loss 4.090487491349499e-09 Test Loss 0.000637217162106217\n",
            "Epoch 9499 Train Loss 4.132480874141956e-09 Test Loss 0.0006152746612005864\n",
            "Epoch 9599 Train Loss 4.162093637562194e-09 Test Loss 0.0006034915251543771\n",
            "Epoch 9699 Train Loss 4.193140558324016e-09 Test Loss 0.0006095298953820031\n",
            "Epoch 9799 Train Loss 4.213182665496872e-09 Test Loss 0.0006278822995580603\n",
            "Epoch 9899 Train Loss 4.2396720481184684e-09 Test Loss 0.0006602719971705879\n",
            "Epoch 9999 Train Loss 4.261108095809647e-09 Test Loss 0.0006966964578766036\n",
            "Epoch 10099 Train Loss 4.283637010547372e-09 Test Loss 0.0007319968883441331\n",
            "Epoch 10199 Train Loss 4.306942738791274e-09 Test Loss 0.0007649599694446013\n",
            "Epoch 10299 Train Loss 4.329658314538362e-09 Test Loss 0.000794674642012074\n",
            "Epoch 10399 Train Loss 4.348351284437048e-09 Test Loss 0.0008291635426788702\n",
            "Epoch 10499 Train Loss 4.369223428774846e-09 Test Loss 0.0008631403689072163\n",
            "Epoch 10599 Train Loss 4.383929926726513e-09 Test Loss 0.0008839316961598034\n",
            "Epoch 10699 Train Loss 4.3984075463663e-09 Test Loss 0.0009052146008248113\n",
            "Epoch 10799 Train Loss 4.415649129549683e-09 Test Loss 0.0009224335970438342\n",
            "Epoch 10899 Train Loss 4.4249739230640105e-09 Test Loss 0.0009424134541988209\n",
            "Epoch 10999 Train Loss 4.434580403050551e-09 Test Loss 0.0009507352809063139\n",
            "Epoch 11099 Train Loss 4.458144777255953e-09 Test Loss 0.0009620593085976883\n",
            "Epoch 11199 Train Loss 4.462730451646864e-09 Test Loss 0.00095391471671187\n",
            "Epoch 11299 Train Loss 4.480197147858676e-09 Test Loss 0.0009410486700658733\n",
            "Epoch 11399 Train Loss 4.490572832526254e-09 Test Loss 0.0009124473002402909\n",
            "Epoch 11499 Train Loss 4.512901057573106e-09 Test Loss 0.0009004116135966908\n",
            "Epoch 11599 Train Loss 4.525389216974825e-09 Test Loss 0.0008792073858419239\n",
            "Epoch 11699 Train Loss 4.519118538259753e-09 Test Loss 0.0008630537848992051\n",
            "Epoch 11799 Train Loss 4.5238630171557076e-09 Test Loss 0.000849477412547435\n",
            "Epoch 11899 Train Loss 4.529605374022422e-09 Test Loss 0.0008256299179541396\n",
            "Epoch 11999 Train Loss 4.546484927003977e-09 Test Loss 0.0008099974261297799\n",
            "Epoch 12099 Train Loss 4.557928372497945e-09 Test Loss 0.000797971445986131\n",
            "Epoch 12199 Train Loss 4.570982139038781e-09 Test Loss 0.0007690656331846168\n",
            "Epoch 12299 Train Loss 4.56748544261163e-09 Test Loss 0.000751961664377774\n",
            "Epoch 12399 Train Loss 4.575197648737689e-09 Test Loss 0.000727400867999879\n",
            "Epoch 12499 Train Loss 4.5973428629624966e-09 Test Loss 0.0007002251974595206\n",
            "Epoch 12599 Train Loss 4.587513720358318e-09 Test Loss 0.0006851621446352379\n",
            "Epoch 12699 Train Loss 4.604100044320868e-09 Test Loss 0.0006638014104263443\n",
            "Epoch 12799 Train Loss 4.606891662127155e-09 Test Loss 0.0006453200377681045\n",
            "Epoch 12899 Train Loss 4.619137364334747e-09 Test Loss 0.0006226196390285154\n",
            "Epoch 12999 Train Loss 4.6265133510671004e-09 Test Loss 0.0006018476602683145\n",
            "Epoch 13099 Train Loss 4.629963424897087e-09 Test Loss 0.0005849924680442639\n",
            "Epoch 13199 Train Loss 4.635571701146934e-09 Test Loss 0.0005636815927503407\n",
            "Epoch 13299 Train Loss 4.646072735953403e-09 Test Loss 0.000548813153541721\n",
            "Epoch 13399 Train Loss 4.67067199446374e-09 Test Loss 0.0005259848029789192\n",
            "Epoch 13499 Train Loss 4.6544687116461765e-09 Test Loss 0.0005056939611954555\n",
            "Epoch 13599 Train Loss 4.66990667441062e-09 Test Loss 0.0004900970201649422\n",
            "Epoch 13699 Train Loss 4.672374286919954e-09 Test Loss 0.00046721387348069886\n",
            "Epoch 13799 Train Loss 4.676191032892606e-09 Test Loss 0.000450538106199148\n",
            "Epoch 13899 Train Loss 4.684021742087407e-09 Test Loss 0.0004302032640289279\n",
            "Epoch 13999 Train Loss 4.688872184141905e-09 Test Loss 0.0004135296286226885\n",
            "Epoch 14099 Train Loss 4.695113140308209e-09 Test Loss 0.0003971246733977124\n",
            "Epoch 14199 Train Loss 4.710880578982014e-09 Test Loss 0.0003766329214139196\n",
            "Epoch 14299 Train Loss 4.7089732947619465e-09 Test Loss 0.0003572157298584408\n",
            "Epoch 14399 Train Loss 4.717709254971532e-09 Test Loss 0.0003443390502011546\n",
            "Epoch 14499 Train Loss 4.728903934253101e-09 Test Loss 0.00032674282246129247\n",
            "Epoch 14599 Train Loss 4.733341780953764e-09 Test Loss 0.00030621595416101233\n",
            "Epoch 14699 Train Loss 4.7401352204879434e-09 Test Loss 0.0002925242437532299\n",
            "Epoch 14799 Train Loss 4.754280943291747e-09 Test Loss 0.0002766503323807078\n",
            "Epoch 14899 Train Loss 4.758506574751244e-09 Test Loss 0.0002574215300315143\n",
            "Epoch 14999 Train Loss 4.766720787774946e-09 Test Loss 0.00023919304664607758\n",
            "Epoch 15099 Train Loss 4.775406491341696e-09 Test Loss 0.00022120514519747208\n",
            "Epoch 15199 Train Loss 4.779585750068768e-09 Test Loss 0.00020212935634236867\n",
            "Epoch 15299 Train Loss 4.790651593695227e-09 Test Loss 0.00018178531063233426\n",
            "Epoch 15399 Train Loss 4.799090286070626e-09 Test Loss 0.0001653942619942384\n",
            "Epoch 15499 Train Loss 4.800080469613963e-09 Test Loss 0.00014825519071873832\n",
            "Epoch 15599 Train Loss 4.811851682008618e-09 Test Loss 0.00013243513063206924\n",
            "Epoch 15699 Train Loss 4.819184596272775e-09 Test Loss 0.00011860593130206625\n",
            "Epoch 15799 Train Loss 4.8340991160247365e-09 Test Loss 0.00010497593890051241\n",
            "Epoch 15899 Train Loss 4.8300589347251586e-09 Test Loss 9.291586869811338e-05\n",
            "Epoch 15999 Train Loss 4.835429983572022e-09 Test Loss 8.228084471848305e-05\n",
            "Epoch 16099 Train Loss 4.839186940421076e-09 Test Loss 7.191984934651871e-05\n",
            "Epoch 16199 Train Loss 4.8431851908956385e-09 Test Loss 6.322355816470365e-05\n",
            "Epoch 16299 Train Loss 4.846144538765383e-09 Test Loss 5.5719307253991584e-05\n",
            "Epoch 16399 Train Loss 4.852807127308739e-09 Test Loss 4.894832696254753e-05\n",
            "Epoch 16499 Train Loss 4.849752391552436e-09 Test Loss 4.271214555353321e-05\n",
            "Epoch 16599 Train Loss 4.850276827126537e-09 Test Loss 3.769579821437805e-05\n",
            "Epoch 16699 Train Loss 4.852533264119369e-09 Test Loss 3.341458826399938e-05\n",
            "Epoch 16799 Train Loss 4.8506384433819674e-09 Test Loss 2.9628389806510206e-05\n",
            "Epoch 16899 Train Loss 4.850099308288173e-09 Test Loss 2.6522892001393167e-05\n",
            "Epoch 16999 Train Loss 4.847378699837567e-09 Test Loss 2.3762233636408143e-05\n",
            "Epoch 17099 Train Loss 4.842468731681922e-09 Test Loss 2.1164374251077834e-05\n",
            "Epoch 17199 Train Loss 4.8423048576018585e-09 Test Loss 1.8833251453171318e-05\n",
            "Epoch 17299 Train Loss 4.84052176158334e-09 Test Loss 1.6962596279908676e-05\n",
            "Epoch 17399 Train Loss 4.837291292704826e-09 Test Loss 1.5360212849118537e-05\n",
            "Epoch 17499 Train Loss 4.836987536237713e-09 Test Loss 1.3779222537008435e-05\n",
            "Epoch 17599 Train Loss 4.835081551854758e-09 Test Loss 1.2493289877844587e-05\n",
            "Epoch 17699 Train Loss 4.833595141710084e-09 Test Loss 1.1409800919818914e-05\n",
            "Epoch 17799 Train Loss 4.832609968527588e-09 Test Loss 1.0513521912016422e-05\n",
            "Epoch 17899 Train Loss 4.831766534384828e-09 Test Loss 9.735144744519057e-06\n",
            "Epoch 17999 Train Loss 4.831922773696194e-09 Test Loss 9.06090663690647e-06\n",
            "Epoch 18099 Train Loss 4.8300743062814844e-09 Test Loss 8.484753974471973e-06\n",
            "Epoch 18199 Train Loss 4.830217048659926e-09 Test Loss 7.991999107569247e-06\n",
            "Epoch 18299 Train Loss 4.829694507119644e-09 Test Loss 7.5980704531810774e-06\n",
            "Epoch 18399 Train Loss 4.828569962415199e-09 Test Loss 7.266168844338969e-06\n",
            "Epoch 18499 Train Loss 4.8287715503571465e-09 Test Loss 6.974348423841542e-06\n",
            "Epoch 18599 Train Loss 4.829252315338805e-09 Test Loss 6.7397795665385775e-06\n",
            "Epoch 18699 Train Loss 4.828597396827074e-09 Test Loss 6.542675504512631e-06\n",
            "Epoch 18799 Train Loss 4.828803230368383e-09 Test Loss 6.385728254259711e-06\n",
            "Epoch 18899 Train Loss 4.827759486841871e-09 Test Loss 6.24797286259355e-06\n",
            "Epoch 18999 Train Loss 4.827213687938922e-09 Test Loss 6.113921982831873e-06\n",
            "Epoch 19099 Train Loss 4.828135490866815e-09 Test Loss 6.001919914423226e-06\n",
            "Epoch 19199 Train Loss 4.827789889999317e-09 Test Loss 5.911979005476473e-06\n",
            "Epoch 19299 Train Loss 4.82739377205755e-09 Test Loss 5.831896989719903e-06\n",
            "Epoch 19399 Train Loss 4.826966437699256e-09 Test Loss 5.768936656358454e-06\n",
            "Epoch 19499 Train Loss 4.827367255032044e-09 Test Loss 5.7116878025094855e-06\n",
            "Epoch 19599 Train Loss 4.828080600126069e-09 Test Loss 5.6855511012196e-06\n",
            "Epoch 19699 Train Loss 4.827093933490892e-09 Test Loss 5.657605090226656e-06\n",
            "Epoch 19799 Train Loss 4.827338167255519e-09 Test Loss 5.639809743878607e-06\n",
            "Epoch 19899 Train Loss 4.826594147509565e-09 Test Loss 5.622133238446189e-06\n",
            "Epoch 19999 Train Loss 4.826462067365673e-09 Test Loss 5.606641018924784e-06\n",
            "Epoch 20099 Train Loss 4.826432761323556e-09 Test Loss 5.591171884548885e-06\n",
            "Epoch 20199 Train Loss 4.8259460912637955e-09 Test Loss 5.581683666347914e-06\n",
            "Epoch 20299 Train Loss 4.825915629054351e-09 Test Loss 5.572651743067414e-06\n",
            "Epoch 20399 Train Loss 4.825648572589462e-09 Test Loss 5.566275476983444e-06\n",
            "Epoch 20499 Train Loss 4.825609251561291e-09 Test Loss 5.558122006574693e-06\n",
            "Epoch 20599 Train Loss 4.8255438020049235e-09 Test Loss 5.5465844902898545e-06\n",
            "Epoch 20699 Train Loss 4.824673850422737e-09 Test Loss 5.539407882190714e-06\n",
            "Epoch 20799 Train Loss 4.8247660984700625e-09 Test Loss 5.539351656617147e-06\n",
            "Epoch 20899 Train Loss 4.826153172794098e-09 Test Loss 5.549575776338477e-06\n",
            "Epoch 20999 Train Loss 4.8246936334462905e-09 Test Loss 5.541341086272497e-06\n",
            "Epoch 21099 Train Loss 4.82403061402828e-09 Test Loss 5.537382768895874e-06\n",
            "Epoch 21199 Train Loss 4.823333221699935e-09 Test Loss 5.537806135091508e-06\n",
            "Epoch 21299 Train Loss 4.824369490448385e-09 Test Loss 5.53725724400424e-06\n",
            "Epoch 21399 Train Loss 4.8239524122076345e-09 Test Loss 5.53822106006995e-06\n",
            "Epoch 21499 Train Loss 4.8235385440568455e-09 Test Loss 5.540067658152909e-06\n",
            "Epoch 21599 Train Loss 4.823629748867528e-09 Test Loss 5.5417772136750525e-06\n",
            "Epoch 21699 Train Loss 4.8232528607085205e-09 Test Loss 5.5440561277054164e-06\n",
            "Epoch 21799 Train Loss 4.823127681145696e-09 Test Loss 5.544496448200728e-06\n",
            "Epoch 21899 Train Loss 4.822723282413553e-09 Test Loss 5.546239039556661e-06\n",
            "Epoch 21999 Train Loss 4.8224613903802195e-09 Test Loss 5.54717816405503e-06\n",
            "Epoch 22099 Train Loss 4.823480594236681e-09 Test Loss 5.55408501939439e-06\n",
            "Epoch 22199 Train Loss 4.8227725188838186e-09 Test Loss 5.5527901359860504e-06\n",
            "Epoch 22299 Train Loss 4.822268085238638e-09 Test Loss 5.553982545746156e-06\n",
            "Epoch 22399 Train Loss 4.82196382097626e-09 Test Loss 5.558038496361001e-06\n",
            "Epoch 22499 Train Loss 4.822114669641681e-09 Test Loss 5.558228864493006e-06\n",
            "Epoch 22599 Train Loss 4.821735482550801e-09 Test Loss 5.561843514061452e-06\n",
            "Epoch 22699 Train Loss 4.821911194469428e-09 Test Loss 5.566801924102055e-06\n",
            "Epoch 22799 Train Loss 4.821639348728857e-09 Test Loss 5.5709641858545805e-06\n",
            "Epoch 22899 Train Loss 4.821323538133049e-09 Test Loss 5.573150787909319e-06\n",
            "Epoch 22999 Train Loss 4.821910997317772e-09 Test Loss 5.5771217244166165e-06\n",
            "Epoch 23099 Train Loss 4.821139970427529e-09 Test Loss 5.579809102987985e-06\n",
            "Epoch 23199 Train Loss 4.8213581885615304e-09 Test Loss 5.5826274545698605e-06\n",
            "Epoch 23299 Train Loss 4.82049284247896e-09 Test Loss 5.581365583402795e-06\n",
            "Epoch 23399 Train Loss 4.821123242406358e-09 Test Loss 5.588347525949735e-06\n",
            "Epoch 23499 Train Loss 4.820972181251437e-09 Test Loss 5.59188052315447e-06\n",
            "Epoch 23599 Train Loss 4.819746983178185e-09 Test Loss 5.591473664621087e-06\n",
            "Epoch 23699 Train Loss 4.818963724690845e-09 Test Loss 5.5984531093714586e-06\n",
            "Epoch 23799 Train Loss 4.819749766386999e-09 Test Loss 5.602832144224432e-06\n",
            "Epoch 23899 Train Loss 4.8193389226300615e-09 Test Loss 5.597251137822937e-06\n",
            "Epoch 23999 Train Loss 4.81880317111892e-09 Test Loss 5.607924289937685e-06\n",
            "Epoch 24099 Train Loss 4.8191618994734486e-09 Test Loss 5.608546775081922e-06\n",
            "Epoch 24199 Train Loss 4.819396204379597e-09 Test Loss 5.613199459002136e-06\n",
            "Epoch 24299 Train Loss 4.818943576034071e-09 Test Loss 5.612932322016328e-06\n",
            "Epoch 24399 Train Loss 4.8188504704447295e-09 Test Loss 5.617866206239973e-06\n",
            "Epoch 24499 Train Loss 4.8185330385642694e-09 Test Loss 5.61404650817966e-06\n",
            "Epoch 24599 Train Loss 4.818520965823431e-09 Test Loss 5.62158242726994e-06\n",
            "Epoch 24699 Train Loss 4.81877036784594e-09 Test Loss 5.62443997698848e-06\n",
            "Epoch 24799 Train Loss 4.816260387207796e-09 Test Loss 5.627748569211146e-06\n",
            "Epoch 24899 Train Loss 4.818411220272702e-09 Test Loss 5.626076863240862e-06\n",
            "Epoch 24999 Train Loss 4.818003041046317e-09 Test Loss 5.627138718408173e-06\n",
            "Epoch 25099 Train Loss 4.816589332884932e-09 Test Loss 5.637295428996712e-06\n",
            "Epoch 25199 Train Loss 4.816480171161728e-09 Test Loss 5.631804393608128e-06\n",
            "Epoch 25299 Train Loss 4.8168945733506764e-09 Test Loss 5.6342750982912045e-06\n",
            "Epoch 25399 Train Loss 4.817174868293873e-09 Test Loss 5.6368757909990955e-06\n",
            "Epoch 25499 Train Loss 4.816683872176755e-09 Test Loss 5.642039253684967e-06\n",
            "Epoch 25599 Train Loss 4.8171784899097804e-09 Test Loss 5.642788649999838e-06\n",
            "Epoch 25699 Train Loss 4.816658802196793e-09 Test Loss 5.643947248103138e-06\n",
            "Epoch 25799 Train Loss 4.816606043617145e-09 Test Loss 5.645872466332237e-06\n",
            "Epoch 25899 Train Loss 4.816608459317994e-09 Test Loss 5.646029414251761e-06\n",
            "Epoch 25999 Train Loss 4.814920652401918e-09 Test Loss 5.652506065027771e-06\n",
            "Epoch 26099 Train Loss 4.81532408828445e-09 Test Loss 5.653661844420421e-06\n",
            "Epoch 26199 Train Loss 4.816011400536107e-09 Test Loss 5.657106673043333e-06\n",
            "Epoch 26299 Train Loss 4.816924991532532e-09 Test Loss 5.662000313472771e-06\n",
            "Epoch 26399 Train Loss 4.8154336079368084e-09 Test Loss 5.662966602238912e-06\n",
            "Epoch 26499 Train Loss 4.815208012556202e-09 Test Loss 5.665654313805582e-06\n",
            "Epoch 26599 Train Loss 4.815686853637273e-09 Test Loss 5.666056948806708e-06\n",
            "Epoch 26699 Train Loss 4.815982224627707e-09 Test Loss 5.671662080819581e-06\n",
            "Epoch 26799 Train Loss 4.8143529628041654e-09 Test Loss 5.669547951688487e-06\n",
            "Epoch 26899 Train Loss 4.816450796168396e-09 Test Loss 5.677599853125626e-06\n",
            "Epoch 26999 Train Loss 4.8155811508194866e-09 Test Loss 5.67795746378039e-06\n",
            "Epoch 27099 Train Loss 4.812803325049255e-09 Test Loss 5.6763464669746e-06\n",
            "Epoch 27199 Train Loss 4.815723879565545e-09 Test Loss 5.683716285768834e-06\n",
            "Epoch 27299 Train Loss 4.8159733083758e-09 Test Loss 5.683446088590514e-06\n",
            "Epoch 27399 Train Loss 4.813723207965461e-09 Test Loss 5.684879969637308e-06\n",
            "Epoch 27499 Train Loss 4.813657565071354e-09 Test Loss 5.686519501105672e-06\n",
            "Epoch 27599 Train Loss 4.814422319212392e-09 Test Loss 5.690794304704297e-06\n",
            "Epoch 27699 Train Loss 4.8139930285155656e-09 Test Loss 5.692951218781862e-06\n",
            "Epoch 27799 Train Loss 4.8145858933521495e-09 Test Loss 5.694264632671936e-06\n",
            "Epoch 27899 Train Loss 4.8130566473159566e-09 Test Loss 5.697090911058939e-06\n",
            "Epoch 27999 Train Loss 4.812637440503332e-09 Test Loss 5.700915711443562e-06\n",
            "Epoch 28099 Train Loss 4.8133079998828294e-09 Test Loss 5.705117624214417e-06\n",
            "Epoch 28199 Train Loss 4.813905833891866e-09 Test Loss 5.708958956951503e-06\n",
            "Epoch 28299 Train Loss 4.8140595653108196e-09 Test Loss 5.71005762950554e-06\n",
            "Epoch 28399 Train Loss 4.81346977659297e-09 Test Loss 5.714248019239007e-06\n",
            "Epoch 28499 Train Loss 4.813564061315341e-09 Test Loss 5.715190642164521e-06\n",
            "Epoch 28599 Train Loss 4.8142630175357794e-09 Test Loss 5.715937595954464e-06\n",
            "Epoch 28699 Train Loss 4.8122925229097184e-09 Test Loss 5.719887223376987e-06\n",
            "Epoch 28799 Train Loss 4.8133630688313545e-09 Test Loss 5.724235518930878e-06\n",
            "Epoch 28899 Train Loss 4.812942778608914e-09 Test Loss 5.725964999613871e-06\n",
            "Epoch 28999 Train Loss 4.813738167239844e-09 Test Loss 5.730476559771713e-06\n",
            "Epoch 29099 Train Loss 4.812629178933988e-09 Test Loss 5.727919612486088e-06\n",
            "Epoch 29199 Train Loss 4.813879301471128e-09 Test Loss 5.735945746109267e-06\n",
            "Epoch 29299 Train Loss 4.813346180048366e-09 Test Loss 5.737707733858154e-06\n",
            "Epoch 29399 Train Loss 4.812004977071685e-09 Test Loss 5.743310396235693e-06\n",
            "Epoch 29499 Train Loss 4.812361550070047e-09 Test Loss 5.7466207882922905e-06\n",
            "Epoch 29599 Train Loss 4.811189868585557e-09 Test Loss 5.7511796516163826e-06\n",
            "Epoch 29699 Train Loss 4.81201111009355e-09 Test Loss 5.7442795223640635e-06\n",
            "Epoch 29799 Train Loss 4.8119715517030825e-09 Test Loss 5.7529836574334404e-06\n",
            "Epoch 29899 Train Loss 4.811556048826206e-09 Test Loss 5.7542415304217356e-06\n",
            "Epoch 29999 Train Loss 4.8118290775774386e-09 Test Loss 5.752997431506392e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
        "from neel_plotly.plot import line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOXORjiYIlub",
        "outputId": "16f4e16d-ce61-4d3a-8457-275137903b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
            "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-m0si6srs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-m0si6srs\n",
            "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (5.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neel_plotly==0.0.0) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neel_plotly==0.0.0) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->neel_plotly==0.0.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->neel_plotly==0.0.0) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->neel_plotly==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->neel_plotly==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->neel_plotly==0.0.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->neel_plotly==0.0.0) (3.0.2)\n",
            "Building wheels for collected packages: neel_plotly\n",
            "  Building wheel for neel_plotly (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neel_plotly: filename=neel_plotly-0.0.0-py3-none-any.whl size=10188 sha256=ed727d03e1ec451ba8b4a0e2199eea084869af21e9e83cd96358fd62bb13c797\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-az0v_m_b/wheels/8b/67/f8/8a94ccda37eea5ac48d867ea4bfa87a4431501b03cfe31f7c2\n",
            "Successfully built neel_plotly\n",
            "Installing collected packages: neel_plotly\n",
            "Successfully installed neel_plotly-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subtitle = f\"(frac_train={frac_train}, lr={lr}, wd={wd})\"\n",
        "\n",
        "fig = line(\n",
        "    [train_losses[::100], test_losses[::100]],\n",
        "    x=np.arange(0, len(train_losses), 100),\n",
        "    xaxis=\"Epoch\",\n",
        "    yaxis=\"Loss\",\n",
        "    log_y=True,\n",
        "    # add the hyper-params right in the title\n",
        "    title=f\"Training Curve for XOR {subtitle}\",\n",
        "    line_labels=[\"train\", \"test\"],\n",
        "    toggle_x=True,\n",
        "    toggle_y=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "f_aaOjMx5UoF",
        "outputId": "0654f038-e859-428d-8b68-f4eb3959fe7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1b9b0186-d15f-4cb5-ad7c-c330e7c64265\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1b9b0186-d15f-4cb5-ad7c-c330e7c64265\")) {                    Plotly.newPlot(                        \"1b9b0186-d15f-4cb5-ad7c-c330e7c64265\",                        [{\"hovertemplate\":\"Color=train\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,20100,20200,20300,20400,20500,20600,20700,20800,20900,21000,21100,21200,21300,21400,21500,21600,21700,21800,21900,22000,22100,22200,22300,22400,22500,22600,22700,22800,22900,23000,23100,23200,23300,23400,23500,23600,23700,23800,23900,24000,24100,24200,24300,24400,24500,24600,24700,24800,24900,25000,25100,25200,25300,25400,25500,25600,25700,25800,25900,26000,26100,26200,26300,26400,26500,26600,26700,26800,26900,27000,27100,27200,27300,27400,27500,27600,27700,27800,27900,28000,28100,28200,28300,28400,28500,28600,28700,28800,28900,29000,29100,29200,29300,29400,29500,29600,29700,29800,29900],\"xaxis\":\"x\",\"y\":[0.726405183216106,0.026291541052917054,0.0003676320132292171,0.00011978661285612961,0.00003784454960357935,0.000012263463022642193,3.965518785909639e-6,1.314168436967854e-6,4.684565573119527e-7,1.73187460156169e-7,6.967274582645675e-8,3.099997554087636e-8,1.5388202065961493e-8,8.408368702339103e-9,5.678389232446625e-9,4.768182652806884e-9,4.7397888703338765e-9,5.001681479195096e-9,5.306323518824003e-9,5.50879320694671e-9,5.6734351545275494e-9,5.7722687549345195e-9,5.604684457781339e-9,5.672376149476681e-9,5.752633056901914e-9,5.8017667104511165e-9,5.846221886950922e-9,5.865118066923734e-9,5.869084205093181e-9,5.8635289482797025e-9,5.839472540446397e-9,5.830241517276526e-9,5.809394391161126e-9,5.7792749084697975e-9,5.769661921996161e-9,5.7704529840522285e-9,5.704800770573975e-9,5.692091308993666e-9,5.686408282175337e-9,5.648892130535691e-9,5.6409291017661625e-9,5.616493868942868e-9,5.58178335178257e-9,5.5716889228454425e-9,5.5596862596267105e-9,5.54182689302736e-9,5.5381005403287866e-9,5.512983603856313e-9,5.5067349910266786e-9,5.49860778947782e-9,5.494235286786002e-9,5.4665171706123625e-9,5.448403654352796e-9,5.445895970453832e-9,5.431193599184984e-9,5.4088949653491166e-9,5.387394270916189e-9,5.39228194544664e-9,5.3978325585548145e-9,5.3811708524804055e-9,5.385040911114109e-9,5.37557549020427e-9,5.354921269097912e-9,5.358040661452717e-9,5.346604442046354e-9,5.339001743823411e-9,5.319899999018721e-9,4.828104111607885e-9,5.029289197950839e-9,5.024500862746516e-9,5.0271401222298934e-9,5.023908449889804e-9,5.021073890746912e-9,5.041246622748604e-9,5.020039134239731e-9,5.0336254313160235e-9,5.05122263459162e-9,5.035984028431704e-9,5.0344227993691405e-9,5.021493867316773e-9,5.0200663469513134e-9,5.007114813650691e-9,4.3285909666507625e-9,4.311061226245468e-9,4.532448488702591e-9,4.569733163747199e-9,4.544443573715426e-9,3.614485475810202e-9,3.791961361326501e-9,3.805444438944218e-9,3.84720733703e-9,3.931968529642897e-9,3.989933445932574e-9,4.056191638624288e-9,4.091956431233448e-9,4.132275890597266e-9,4.162812298354546e-9,4.192877129211206e-9,4.215972493171881e-9,4.2374153035788864e-9,4.260417029902568e-9,4.282262458830218e-9,4.305816811911865e-9,4.329195087432526e-9,4.348287712719091e-9,4.366911153926974e-9,4.3984411573160725e-9,4.40052683577506e-9,4.419112965930762e-9,4.425144039859351e-9,4.4345620250755856e-9,4.457318541666552e-9,4.463230654927154e-9,4.4852350985684024e-9,4.481627513466186e-9,4.5004976196870834e-9,4.5167356701902e-9,4.517258650584768e-9,4.524305961638313e-9,4.533143382824079e-9,4.55786618822452e-9,4.563257006300108e-9,4.559213808124854e-9,4.572735196986343e-9,4.57651994353786e-9,4.587983625984391e-9,4.5852164969175285e-9,4.604219580948415e-9,4.607312568704276e-9,4.61511071574217e-9,4.62302010341872e-9,4.629959347690551e-9,4.634159838239376e-9,4.643526246191469e-9,4.654849404934125e-9,4.653903682757387e-9,4.663311754759828e-9,4.671825725739348e-9,4.678374364921987e-9,4.6894951834553154e-9,4.690925353957455e-9,4.6945607737640815e-9,4.702897618352467e-9,4.71196962770071e-9,4.716510773130099e-9,4.729250006090601e-9,4.7332057576417116e-9,4.740680998467912e-9,4.755198701546594e-9,4.754125870332835e-9,4.765164061838551e-9,4.771192902275424e-9,4.780303298673628e-9,4.7901288413812106e-9,4.793636183644418e-9,4.805375050266033e-9,4.812340706517068e-9,4.82084558747011e-9,4.823718841271354e-9,4.829186980899203e-9,4.835469059607424e-9,4.840766795390928e-9,4.8455909924296765e-9,4.84705158521741e-9,4.848741594086641e-9,4.85193771046686e-9,4.851092226574122e-9,4.852501429435343e-9,4.851523748600079e-9,4.850323602047933e-9,4.847563460089326e-9,4.8437139271669986e-9,4.8435842345671234e-9,4.841101431983626e-9,4.837369609367215e-9,4.837588132434719e-9,4.835437276876652e-9,4.834738055037257e-9,4.832780161886803e-9,4.832067554126766e-9,4.831915074876817e-9,4.830274385696174e-9,4.8298464466571985e-9,4.8296453510381476e-9,4.8283190654171885e-9,4.829094049083513e-9,4.8287770709873215e-9,4.828690289948155e-9,4.828369077663407e-9,4.828488690883468e-9,4.8279249216745254e-9,4.828245885319435e-9,4.8277342582536875e-9,4.828084443698236e-9,4.827713418699092e-9,4.8277283876021075e-9,4.827867282260781e-9,4.827522043137419e-9,4.827343720287125e-9,4.826782916045589e-9,4.826251695418774e-9,4.826650336322096e-9,4.82534367852171e-9,4.825863165277805e-9,4.8254749815670595e-9,4.825626594013073e-9,4.8256464516955195e-9,4.824913134111445e-9,4.8248884680161155e-9,4.825064300529774e-9,4.825227378894929e-9,4.8246084851002e-9,4.8239885212526096e-9,4.824553370178798e-9,4.824253276897022e-9,4.823280491862491e-9,4.823205873382395e-9,4.822225647931032e-9,4.8229738176341485e-9,4.822365771469657e-9,4.822704247834022e-9,4.822410408175755e-9,4.822357756797256e-9,4.822777252648028e-9,4.822304774282958e-9,4.821939145305014e-9,4.821394412473884e-9,4.8212594706019545e-9,4.821727638252268e-9,4.821149129756125e-9,4.821101110696311e-9,4.820056873234426e-9,4.820919933773918e-9,4.8208187254698406e-9,4.820151914050916e-9,4.82104968247207e-9,4.820463371796218e-9,4.818683102425287e-9,4.819108570493634e-9,4.820020362197613e-9,4.818512380742431e-9,4.819714735012767e-9,4.818322850356905e-9,4.818648203118292e-9,4.818544988757315e-9,4.818280236959919e-9,4.818078580241318e-9,4.818765421603416e-9,4.816738294181288e-9,4.8184717235955655e-9,4.819041227798627e-9,4.816248461878322e-9,4.8166793815368065e-9,4.81695409662093e-9,4.817413468618046e-9,4.817581039554022e-9,4.816101359219177e-9,4.816202077507808e-9,4.817883933258186e-9,4.81759106601477e-9,4.814775054313978e-9,4.8155761031623285e-9,4.8161819824621895e-9,4.8164477372946766e-9,4.814869731418687e-9,4.815425625834225e-9,4.816258053796093e-9,4.81501466719586e-9,4.8150746670927355e-9,4.814061025855503e-9,4.815457640423823e-9,4.8137356309877675e-9,4.8141595964232674e-9,4.817357940220816e-9,4.814732025490719e-9,4.814420837645209e-9,4.8145776202715935e-9,4.814113755699319e-9,4.814967953625705e-9,4.813211473658525e-9,4.813406601084432e-9,4.813106963379589e-9,4.8134351491066685e-9,4.8149822486816475e-9,4.814404530754932e-9,4.812954652253918e-9,4.813756258158258e-9,4.813149306884405e-9,4.8138885258961135e-9,4.813426893262991e-9,4.8129272316516804e-9,4.811968990569091e-9,4.813200585785866e-9,4.812693671365536e-9,4.812385624680479e-9,4.811484822282587e-9,4.8113670812254275e-9,4.8136855273712526e-9,4.812364922848462e-9,4.812345572421693e-9],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Color=test\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,20100,20200,20300,20400,20500,20600,20700,20800,20900,21000,21100,21200,21300,21400,21500,21600,21700,21800,21900,22000,22100,22200,22300,22400,22500,22600,22700,22800,22900,23000,23100,23200,23300,23400,23500,23600,23700,23800,23900,24000,24100,24200,24300,24400,24500,24600,24700,24800,24900,25000,25100,25200,25300,25400,25500,25600,25700,25800,25900,26000,26100,26200,26300,26400,26500,26600,26700,26800,26900,27000,27100,27200,27300,27400,27500,27600,27700,27800,27900,28000,28100,28200,28300,28400,28500,28600,28700,28800,28900,29000,29100,29200,29300,29400,29500,29600,29700,29800,29900],\"xaxis\":\"x\",\"y\":[0.6956655564167412,0.8462517269319807,1.1057834034899392,1.2356517783131202,1.3800344778851839,1.5274375037545005,1.678795021169568,1.8308707006431113,1.9690188774619755,2.08078756194476,2.165256439044468,2.204051694492585,2.191170835927415,2.1366314634377677,2.0427007597904026,1.930513452788061,1.8175050713783476,1.7180405447359095,1.6332641320305312,1.5702638172536405,1.5178349148827,1.4740968187379107,1.382614740512175,1.3008137516902503,1.2462624117789745,1.2107497922758719,1.183065865351007,1.1587898508358736,1.1433634947547795,1.128212333468117,1.111946727899079,1.0968151811828355,1.0809487878347899,1.0600517828610332,1.0368448997527608,1.0148054103315256,0.9964350470830816,0.9737014993157928,0.9535853062449948,0.9311700860429872,0.9086669231340277,0.8868300865823857,0.8657475488603118,0.8411390786191646,0.8220248298934802,0.798450461273102,0.7800103391018258,0.7613306960597933,0.7459314927754138,0.7279523575696468,0.7121552077699659,0.6967384851922683,0.6815643091721143,0.6667459109169925,0.6530229872747922,0.641061515646085,0.6303237354196982,0.6190659695681966,0.6097094771239575,0.5994745989864196,0.5897229596113737,0.5804682523704494,0.5708398070977806,0.5614444104591848,0.5522728580494828,0.5428820383416344,0.5336602448319846,0.5082633288572096,0.4704274036645098,0.4376764400040494,0.41234766301141307,0.39102065403083713,0.3717891023983077,0.3560610064408998,0.3406198512584842,0.3273664227860646,0.31448763101705085,0.30118798787899836,0.28865149258020373,0.27747162914114254,0.2649944917878233,0.25245502746602755,0.21741980801085456,0.15486475218540566,0.11749059304023195,0.09030233232330402,0.07275824856015194,0.051640894124788846,0.028717727729867058,0.014517039513849868,0.0059151320149410744,0.002099840875526647,0.0009839483233076258,0.0007267784799141632,0.0006375056193805858,0.0006158484554792225,0.0006034367658537901,0.0006079959692986512,0.0006276572781746008,0.0006579967736287692,0.0006963199946321137,0.0007315302428394853,0.0007670696887370652,0.0007954902356238899,0.0008286538316944352,0.0008565890477339474,0.0008856117365031132,0.0009079464908778785,0.0009234127257890484,0.0009410836713079215,0.0009516021084743147,0.0009614670675523672,0.0009510433194110069,0.0009398645290162792,0.0009166499345714267,0.0008998727756323163,0.0008807636926261984,0.0008603662779273035,0.0008469483031129417,0.0008287077974234286,0.0008101108458496694,0.0007973616593419464,0.0007713720750669616,0.0007535959811108372,0.0007283290771619113,0.0007030667989832574,0.0006834731790811874,0.0006622289934376874,0.0006474196061839922,0.000621934038877104,0.0005974711105756895,0.0005829920717175468,0.0005635809310749443,0.0005495614172709288,0.0005288196823642183,0.0005048551413348235,0.0004890145774309017,0.000468000533865462,0.000450123251867734,0.00043086004679872557,0.0004134250675481138,0.0003956486490422366,0.00037766193013890004,0.00035793518764881574,0.0003446003060956037,0.0003256634309392561,0.00030461414866867176,0.0002920914613898121,0.0002750770441953017,0.0002576987776448617,0.00023897077518023115,0.0002213798665619877,0.00020186995739545756,0.0001824283658371689,0.00016580249391494244,0.0001481372072630913,0.00013283110162585343,0.00011906184262618746,0.0001051613114529292,0.000092660600001202,0.0000819309650019263,0.00007186025862468175,0.0000633921436806236,0.00005544374854110755,0.000049015035222673517,0.0000427005471549424,0.00003756185233547064,0.00003331698693323523,0.00002957506845359543,0.00002654506859047285,0.00002368877032242754,0.00002106240368282052,0.000018800545738775235,0.00001697004648355517,0.000015350390798739686,0.00001376244787597569,0.000012477641867792857,0.000011404733698454249,0.000010499857249201565,9.71792230507539e-6,9.061219857494215e-6,8.480151828033514e-6,7.987165185467845e-6,7.590786160702952e-6,7.259730821722171e-6,6.974858786747891e-6,6.740195952917382e-6,6.544037300212371e-6,6.38498686713025e-6,6.247358599037895e-6,6.1136099932826536e-6,5.998848977350787e-6,5.911333255690449e-6,5.832012262532569e-6,5.767568613479382e-6,5.71187410851523e-6,5.685516064512527e-6,5.659785725115823e-6,5.6426814807873365e-6,5.620060317215012e-6,5.6065987612457055e-6,5.5935163283865315e-6,5.581119498148916e-6,5.569367220959598e-6,5.564585865133009e-6,5.558135550595981e-6,5.549775101838293e-6,5.544330349680011e-6,5.542316307500519e-6,5.547138700088551e-6,5.5429229719151855e-6,5.538108578594264e-6,5.5367587784035784e-6,5.541724010826747e-6,5.541048538847421e-6,5.540754917056511e-6,5.540170611281581e-6,5.543671888426084e-6,5.542665955570662e-6,5.542818814445767e-6,5.544508489722733e-6,5.553271355399477e-6,5.552185886656171e-6,5.553834099362226e-6,5.555364151575495e-6,5.559014176254212e-6,5.560609451653019e-6,5.56540838834777e-6,5.571686068184612e-6,5.570516816654508e-6,5.578144385130648e-6,5.579290550700171e-6,5.580693315989453e-6,5.583951515712819e-6,5.589139817163546e-6,5.594852105580606e-6,5.594573223581121e-6,5.595052662000992e-6,5.601306277249715e-6,5.599853634725645e-6,5.605363778848016e-6,5.606763302253414e-6,5.611575978963772e-6,5.613186201366868e-6,5.6144505478831924e-6,5.617648933164914e-6,5.618517444407258e-6,5.623373886293174e-6,5.6239033994305365e-6,5.627203430191358e-6,5.62834736620173e-6,5.634866999333182e-6,5.6301009840326166e-6,5.634532203485134e-6,5.6388901960197095e-6,5.640164189474893e-6,5.644046029344003e-6,5.646678921868825e-6,5.646587654904931e-6,5.649758540512911e-6,5.650134488558302e-6,5.652266156236982e-6,5.657003434135893e-6,5.6602713614898515e-6,5.660876567325532e-6,5.6653521450906956e-6,5.664443644938764e-6,5.671329534179707e-6,5.6737262712982984e-6,5.679535510128875e-6,5.676238891829291e-6,5.673832403741023e-6,5.683903753057836e-6,5.686123747312327e-6,5.6821511154057e-6,5.687634224244063e-6,5.694867164719281e-6,5.693662978836462e-6,5.698055344231515e-6,5.700580453170536e-6,5.699669965988817e-6,5.709145938058616e-6,5.706746896976168e-6,5.711573259397463e-6,5.71459557412624e-6,5.715572621497605e-6,5.720514699462486e-6,5.716913080033986e-6,5.7265722245697385e-6,5.72919934876145e-6,5.731906165021964e-6,5.731167845587277e-6,5.736374239838401e-6,5.740334033651182e-6,5.740242665331319e-6,5.744407239230772e-6,5.745004021189793e-6,5.749805224511051e-6,5.75226278563549e-6,5.752565214932475e-6],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Curve for XOR (frac_train=0.115, lr=0.001, wd=1)\"},\"updatemenus\":[{\"active\":-1,\"buttons\":[{\"args\":[{\"xaxis.type\":\"log\"}],\"args2\":[{\"xaxis.type\":\"linear\"}],\"label\":\"Log x-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":1.0},{\"active\":0,\"buttons\":[{\"args\":[{\"yaxis.type\":\"log\"}],\"args2\":[{\"yaxis.type\":\"linear\"}],\"label\":\"Log y-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":0.85}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1b9b0186-d15f-4cb5-ad7c-c330e7c64265');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)"
      ],
      "metadata": {
        "id": "89dWTjbhKkJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Delete the old checkpoint file, if you saved one:\n",
        "if os.path.exists(PTH_LOCATION):\n",
        "    os.remove(PTH_LOCATION)\n",
        "    print(f\"Removed checkpoint file at {PTH_LOCATION}\")\n",
        "\n",
        "# 2) Clear out your history lists:\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "print(\"Cleared loss and checkpoint history.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7aNFhDrI_Cf",
        "outputId": "c827c969-40a1-4e77-8b51-ad4aa55f8f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared loss and checkpoint history.\n"
          ]
        }
      ]
    }
  ]
}