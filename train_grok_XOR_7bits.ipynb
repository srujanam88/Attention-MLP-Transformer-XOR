{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install & import relevant libraries"
      ],
      "metadata": {
        "id": "4ZgnepZyQ1dg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OTQQSIwS7Rxj"
      },
      "outputs": [],
      "source": [
        "%pip -q install transformer_lens\n",
        "%pip -q install circuitsvis\n",
        "%pip -q install git+https://github.com/neelnanda-io/neel-plotly.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "from neel_plotly.plot import line\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ],
      "metadata": {
        "id": "BDL7FPZB8ewF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "wGkh0Co48hLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d3ab24-39c1-4fb2-9d23-0d3bcbaf3819"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/grokking_xor_7bits.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ],
      "metadata": {
        "id": "oQIZVbPC8kl1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to generate the dataset\n",
        "from itertools import product\n",
        "def generate_xor_dataset(bit_length=1, n_samples=None, device='cpu', seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "    if n_samples is None:\n",
        "        all_pairs = list(product([0, 1], repeat=2 * bit_length))\n",
        "        #random.shuffle(all_pairs)\n",
        "        inputs = torch.tensor(all_pairs, dtype=torch.long, device=device)\n",
        "    else:\n",
        "        inputs = torch.randint(\n",
        "            0, 2, size=(n_samples, 2 * bit_length), dtype=torch.long, device=device\n",
        "        )\n",
        "\n",
        "    a = inputs[:, :bit_length]\n",
        "    b = inputs[:, bit_length:]\n",
        "    targets = torch.bitwise_xor(a, b)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "seed = 598\n",
        "dataset, labels = generate_xor_dataset(bit_length=7, device=device, seed=seed)"
      ],
      "metadata": {
        "id": "UAJh9wk08msm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "print(labels.shape)\n",
        "print(dataset)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkrGqZOJ6SBO",
        "outputId": "7d4c10f3-6b49-488c-8b90-7ea611770e73"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 14])\n",
            "torch.Size([16384, 7])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 1],\n",
            "        [0, 0, 0,  ..., 0, 1, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 0, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 1],\n",
            "        [0, 0, 0,  ..., 0, 1, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 1, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 1],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "oGjKQ_YV7y_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30000\n",
        "checkpoint_every = 100\n",
        "n_samples = 16384  # 2^14 = 16384\n",
        "bit_length = 7\n",
        "frac_train = 0.008\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1\n",
        "betas = (0.9, 0.98)"
      ],
      "metadata": {
        "id": "CZ-QN6wZ7g5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into train-test sets\n",
        "torch.manual_seed(seed)\n",
        "indices = torch.randperm(n_samples)\n",
        "cutoff = int(n_samples*frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]\n",
        "print(train_data[:5])\n",
        "print(train_labels[:5])\n",
        "print(train_data.shape)\n",
        "print(test_data[:5])\n",
        "print(test_labels[:5])\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWFsWaLk75U6",
        "outputId": "0569ae33-641d-4fef-f57e-a8b9a5135d5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1],\n",
            "        [0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1],\n",
            "        [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]], device='cuda:0')\n",
            "tensor([[1, 0, 0, 0, 0, 0, 1],\n",
            "        [1, 0, 1, 0, 1, 0, 1],\n",
            "        [0, 1, 0, 0, 1, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 1, 0, 0, 1, 0]], device='cuda:0')\n",
            "torch.Size([131, 14])\n",
            "tensor([[0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
            "        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]], device='cuda:0')\n",
            "tensor([[0, 0, 1, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 1, 0],\n",
            "        [0, 1, 0, 1, 1, 1, 0],\n",
            "        [0, 1, 0, 1, 1, 1, 0]], device='cuda:0')\n",
            "torch.Size([16253, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "9TzmaSZF_JJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 2,\n",
        "    d_model = 64, # n_heads * d_head\n",
        "    d_head = 32,\n",
        "    d_mlp = 256,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=2, # input always consists of 0 or 1\n",
        "    d_vocab_out=2, # output always consist of 0 or 1\n",
        "    n_ctx=14, # 7 bits for input A + 7 bits for input B\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(cfg)"
      ],
      "metadata": {
        "id": "7-KxBipj-bdM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabling the biases makes model easier to interpret.\n",
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "DDO492qp_Vt5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ],
      "metadata": {
        "id": "fY9zsBPd_YFS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Loss function that is consistent with the bit 7-to-13 slice\n",
        "# ------------------------------------------------------------------\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "      logits = logits[:,7:14,:]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))\n",
        "    return -correct_log_probs.mean()"
      ],
      "metadata": {
        "id": "X1MeYPU6_alq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Accuracy function that is consistent with the bit 7-to-13 slice\n",
        "# ------------------------------------------------------------------\n",
        "def accuracy_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    Compute token-level accuracy for the 7-bit XOR target.\n",
        "    Works for batched (B, L, V) and un-batched (L, V) logits.\n",
        "    \"\"\"\n",
        "    # Keep only the 7 positions that correspond to the target\n",
        "    if logits.dim() == 3:          # (batch, seq_len, vocab)\n",
        "        logits = logits[:, 7:14, :]\n",
        "    else:                          # (seq_len, vocab)\n",
        "        logits = logits[7:14, :]\n",
        "\n",
        "    preds = logits.argmax(dim=-1)          # (batch, 7)  or  (7,)\n",
        "    correct = (preds == labels).float()    # element-wise comparison\n",
        "    return correct.mean()                  # scalar tensor"
      ],
      "metadata": {
        "id": "68aWvr2jYUUw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "6QrxtX4SombR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Training loop with accuracy tracking at every checkpoint\n",
        "# ------------------------------------------------------------------\n",
        "train_losses,  test_losses  = [], []\n",
        "train_accs,    test_accs    = [], []\n",
        "model_checkpoints, checkpoint_epochs = [], []\n",
        "\n",
        "for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "\n",
        "    # ------------------- forward / backward pass ------------------\n",
        "    train_logits = model(train_data)\n",
        "    train_loss   = loss_fn(train_logits, train_labels)\n",
        "    train_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    # ------------------- evaluation on test split -----------------\n",
        "    with torch.inference_mode():\n",
        "        test_logits = model(test_data)\n",
        "        test_loss   = loss_fn(test_logits, test_labels)\n",
        "\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # ------------------- checkpoint bookkeeping -------------------\n",
        "    if (epoch + 1) % checkpoint_every == 0:\n",
        "        # accuracies\n",
        "        with torch.inference_mode():\n",
        "            train_acc = accuracy_fn(train_logits, train_labels).item()\n",
        "            test_acc  = accuracy_fn(test_logits,  test_labels).item()\n",
        "\n",
        "        train_accs.append(train_acc)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        checkpoint_epochs.append(epoch)\n",
        "        model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "\n",
        "        print(f\"Epoch {epoch:4d} | \"\n",
        "              f\"Train Loss {train_loss.item()} | \"\n",
        "              f\"Test Loss {test_loss.item()} | \"\n",
        "              f\"Train Acc {train_acc} | \"\n",
        "              f\"Test Acc {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "09a30a007bc3499cbfa93a7c093e8f1a",
            "b14f1c5be2d34495a0d81cbec2cd97f0",
            "6271705bc16c4b2198105c3b281342f2",
            "014505cfa63c40b885c266d6865ad36b",
            "857f1d2bbf18452c847cebd5472350db",
            "a45547a81a5d4449b504e6ef6bcaa784",
            "2001ea172c304beea81751f891970959",
            "ec377617f6bb47a28ac3bfa886273f5b",
            "ba10dce8e2d343b5b8984785b6f7a112",
            "b2c29b3e3b4d4f4094d7f20e638ed9f7",
            "98f23fa5b98b4394b26442801413384f"
          ]
        },
        "id": "39f8S-ry_dtO",
        "outputId": "0b2d2501-03c6-4f79-fc98-6423be02b8ac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09a30a007bc3499cbfa93a7c093e8f1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   99 | Train Loss 0.18306178814700577 | Test Loss 0.24012129789610476 | Train Acc 0.9182115197181702 | Test Acc 0.8943843245506287\n",
            "Epoch  199 | Train Loss 0.0005480773068867601 | Test Loss 0.009368215122606709 | Train Acc 0.9999999403953552 | Test Acc 0.9963698983192444\n",
            "Epoch  299 | Train Loss 0.00018905175646199556 | Test Loss 0.008018478706503636 | Train Acc 0.9999999403953552 | Test Acc 0.9962908029556274\n",
            "Epoch  399 | Train Loss 5.648530365646469e-05 | Test Loss 0.007138387216611768 | Train Acc 0.9999999403953552 | Test Acc 0.9969412088394165\n",
            "Epoch  499 | Train Loss 1.8314097808615913e-05 | Test Loss 0.00604788142019239 | Train Acc 0.9999999403953552 | Test Acc 0.9975565075874329\n",
            "Epoch  599 | Train Loss 6.105455986420531e-06 | Test Loss 0.004952785776131844 | Train Acc 0.9999999403953552 | Test Acc 0.9983211755752563\n",
            "Epoch  699 | Train Loss 2.077537797763248e-06 | Test Loss 0.0039571269896723806 | Train Acc 0.9999999403953552 | Test Acc 0.9987342953681946\n",
            "Epoch  799 | Train Loss 7.340297673877721e-07 | Test Loss 0.0031107229479899754 | Train Acc 0.9999999403953552 | Test Acc 0.9991561770439148\n",
            "Epoch  899 | Train Loss 2.7028095127640476e-07 | Test Loss 0.0023413558039527874 | Train Acc 0.9999999403953552 | Test Acc 0.9995341300964355\n",
            "Epoch  999 | Train Loss 1.0657441958986526e-07 | Test Loss 0.0016908221446226446 | Train Acc 0.9999999403953552 | Test Acc 0.99964839220047\n",
            "Epoch 1099 | Train Loss 4.5548605499793435e-08 | Test Loss 0.001500840361575997 | Train Acc 0.9999999403953552 | Test Acc 0.9996659755706787\n",
            "Epoch 1199 | Train Loss 2.1536712489561317e-08 | Test Loss 0.0034095916270786312 | Train Acc 0.9999999403953552 | Test Acc 0.999279260635376\n",
            "Epoch 1299 | Train Loss 1.18390514620364e-08 | Test Loss 0.008972810962474343 | Train Acc 0.9999999403953552 | Test Acc 0.9985057711601257\n",
            "Epoch 1399 | Train Loss 7.863325270156184e-09 | Test Loss 0.012096239810153326 | Train Acc 0.9999999403953552 | Test Acc 0.9982244968414307\n",
            "Epoch 1499 | Train Loss 6.3688473295331685e-09 | Test Loss 0.0147512069664824 | Train Acc 0.9999999403953552 | Test Acc 0.9980223178863525\n",
            "Epoch 1599 | Train Loss 6.059439487573971e-09 | Test Loss 0.01679108864703201 | Train Acc 0.9999999403953552 | Test Acc 0.9978377223014832\n",
            "Epoch 1699 | Train Loss 6.222827154860053e-09 | Test Loss 0.018243388450089545 | Train Acc 0.9999999403953552 | Test Acc 0.9977498650550842\n",
            "Epoch 1799 | Train Loss 6.502785411345901e-09 | Test Loss 0.019148071928866455 | Train Acc 0.9999999403953552 | Test Acc 0.9977146983146667\n",
            "Epoch 1899 | Train Loss 6.76827267153238e-09 | Test Loss 0.019010205708604858 | Train Acc 0.9999999403953552 | Test Acc 0.99770587682724\n",
            "Epoch 1999 | Train Loss 6.973003593837524e-09 | Test Loss 0.018328111256207157 | Train Acc 0.9999999403953552 | Test Acc 0.997688353061676\n",
            "Epoch 2099 | Train Loss 7.131263089567967e-09 | Test Loss 0.017535326976295857 | Train Acc 0.9999999403953552 | Test Acc 0.9978553056716919\n",
            "Epoch 2199 | Train Loss 7.241747346325709e-09 | Test Loss 0.01669286860536577 | Train Acc 0.9999999403953552 | Test Acc 0.9979608058929443\n",
            "Epoch 2299 | Train Loss 7.3216978823272546e-09 | Test Loss 0.015866504563707893 | Train Acc 0.9999999403953552 | Test Acc 0.99805748462677\n",
            "Epoch 2399 | Train Loss 7.374388425489172e-09 | Test Loss 0.015069650963804911 | Train Acc 0.9999999403953552 | Test Acc 0.9981541633605957\n",
            "Epoch 2499 | Train Loss 7.40529037131651e-09 | Test Loss 0.014294546596918187 | Train Acc 0.9999999403953552 | Test Acc 0.9982684254646301\n",
            "Epoch 2599 | Train Loss 7.4235421687277335e-09 | Test Loss 0.013539984808950545 | Train Acc 0.9999999403953552 | Test Acc 0.9984002709388733\n",
            "Epoch 2699 | Train Loss 7.429643846070183e-09 | Test Loss 0.01281791752066392 | Train Acc 0.9999999403953552 | Test Acc 0.9985233545303345\n",
            "Epoch 2799 | Train Loss 7.427980079715162e-09 | Test Loss 0.012132188485277454 | Train Acc 0.9999999403953552 | Test Acc 0.998558521270752\n",
            "Epoch 2899 | Train Loss 7.42183184133388e-09 | Test Loss 0.011479701546355017 | Train Acc 0.9999999403953552 | Test Acc 0.9986112117767334\n",
            "Epoch 2999 | Train Loss 7.411534108944991e-09 | Test Loss 0.010844387193926972 | Train Acc 0.9999999403953552 | Test Acc 0.9980926513671875\n",
            "Epoch 3099 | Train Loss 7.399850080477289e-09 | Test Loss 0.010218682961554742 | Train Acc 0.9999999403953552 | Test Acc 0.998127818107605\n",
            "Epoch 3199 | Train Loss 7.385977838200855e-09 | Test Loss 0.0096001954860546 | Train Acc 0.9999999403953552 | Test Acc 0.9982332587242126\n",
            "Epoch 3299 | Train Loss 7.371496325641757e-09 | Test Loss 0.008994182696592922 | Train Acc 0.9999999403953552 | Test Acc 0.9982860088348389\n",
            "Epoch 3399 | Train Loss 7.356716381747818e-09 | Test Loss 0.008396145020633874 | Train Acc 0.9999999403953552 | Test Acc 0.9983915090560913\n",
            "Epoch 3499 | Train Loss 7.3426085683540245e-09 | Test Loss 0.007803225596921981 | Train Acc 0.9999999403953552 | Test Acc 0.9984442591667175\n",
            "Epoch 3599 | Train Loss 7.3275678230309465e-09 | Test Loss 0.00720898531872023 | Train Acc 0.9999999403953552 | Test Acc 0.998488187789917\n",
            "Epoch 3699 | Train Loss 7.312598889562111e-09 | Test Loss 0.00660417577603826 | Train Acc 0.9999999403953552 | Test Acc 0.9985145330429077\n",
            "Epoch 3799 | Train Loss 7.2987952536399535e-09 | Test Loss 0.005992780911284119 | Train Acc 0.9999999403953552 | Test Acc 0.9985848665237427\n",
            "Epoch 3899 | Train Loss 7.283697547346736e-09 | Test Loss 0.005364209502181335 | Train Acc 0.9999999403953552 | Test Acc 0.9986552000045776\n",
            "Epoch 3999 | Train Loss 7.269775227840139e-09 | Test Loss 0.004713769659673464 | Train Acc 0.9999999403953552 | Test Acc 0.9987606406211853\n",
            "Epoch 4099 | Train Loss 7.2563672208578895e-09 | Test Loss 0.004042540110579386 | Train Acc 0.9999999403953552 | Test Acc 0.9988133907318115\n",
            "Epoch 4199 | Train Loss 7.240877309169757e-09 | Test Loss 0.003373210087948605 | Train Acc 0.9999999403953552 | Test Acc 0.9988924860954285\n",
            "Epoch 4299 | Train Loss 7.226244063482682e-09 | Test Loss 0.0027246473201626223 | Train Acc 0.9999999403953552 | Test Acc 0.9990067481994629\n",
            "Epoch 4399 | Train Loss 7.2080267463668335e-09 | Test Loss 0.0021132674712754378 | Train Acc 0.9999999403953552 | Test Acc 0.9990858435630798\n",
            "Epoch 4499 | Train Loss 7.188407018134685e-09 | Test Loss 0.0015659694090961152 | Train Acc 0.9999999403953552 | Test Acc 0.9991825819015503\n",
            "Epoch 4599 | Train Loss 7.167864136973116e-09 | Test Loss 0.0011369437792840469 | Train Acc 0.9999999403953552 | Test Acc 0.9992968440055847\n",
            "Epoch 4699 | Train Loss 7.143205463351476e-09 | Test Loss 0.0008387266483876084 | Train Acc 0.9999999403953552 | Test Acc 0.9993671178817749\n",
            "Epoch 4799 | Train Loss 7.114486549868292e-09 | Test Loss 0.0006543490851603197 | Train Acc 0.9999999403953552 | Test Acc 0.9994198679924011\n",
            "Epoch 4899 | Train Loss 7.07745748782655e-09 | Test Loss 0.0005244333335570505 | Train Acc 0.9999999403953552 | Test Acc 0.9994374513626099\n",
            "Epoch 4999 | Train Loss 7.0377031671331335e-09 | Test Loss 0.00039563992910815883 | Train Acc 0.9999999403953552 | Test Acc 0.9994374513626099\n",
            "Epoch 5099 | Train Loss 6.9852289533515526e-09 | Test Loss 0.00025105599876052814 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5199 | Train Loss 6.932071314253995e-09 | Test Loss 0.00012296718075536787 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5299 | Train Loss 6.8791906706293405e-09 | Test Loss 4.320922284246187e-05 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5399 | Train Loss 6.8240797790592886e-09 | Test Loss 1.069947754572979e-05 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5499 | Train Loss 6.7760721658909324e-09 | Test Loss 2.3295087299567916e-06 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5599 | Train Loss 6.726217578084935e-09 | Test Loss 8.629683500050127e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5699 | Train Loss 6.6803097249474384e-09 | Test Loss 8.126864346238683e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5799 | Train Loss 6.64151797680297e-09 | Test Loss 1.0303613481519197e-06 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5899 | Train Loss 6.605148986122624e-09 | Test Loss 1.1938666129095047e-06 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 5999 | Train Loss 6.5707237991570535e-09 | Test Loss 1.0691433014876137e-06 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6099 | Train Loss 6.5348149076687915e-09 | Test Loss 7.626483262341904e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6199 | Train Loss 6.504412526885937e-09 | Test Loss 4.953442657863299e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6299 | Train Loss 6.468933894314542e-09 | Test Loss 3.205852185718472e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6399 | Train Loss 6.439837108513505e-09 | Test Loss 2.1582019135136225e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6499 | Train Loss 6.410661313600132e-09 | Test Loss 1.51877944295754e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6599 | Train Loss 6.384858265309456e-09 | Test Loss 1.115209197437624e-07 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6699 | Train Loss 6.363452765527356e-09 | Test Loss 8.553025831074694e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6799 | Train Loss 6.34141380378052e-09 | Test Loss 6.809365730393423e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6899 | Train Loss 6.321574498503285e-09 | Test Loss 5.5813607760695266e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 6999 | Train Loss 6.301445308966106e-09 | Test Loss 4.69058713740982e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7099 | Train Loss 6.284320574677803e-09 | Test Loss 4.028107443227232e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7199 | Train Loss 6.269362629963429e-09 | Test Loss 3.5323514125914945e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7299 | Train Loss 6.2554428838786305e-09 | Test Loss 3.157507047502069e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7399 | Train Loss 6.242695338784216e-09 | Test Loss 2.8645582002042557e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7499 | Train Loss 6.2316256232730654e-09 | Test Loss 2.629901312687829e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7599 | Train Loss 6.2203187320432914e-09 | Test Loss 2.4431188339039257e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7699 | Train Loss 6.210294730229501e-09 | Test Loss 2.2952065016579568e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7799 | Train Loss 6.200590009500425e-09 | Test Loss 2.179685413572966e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7899 | Train Loss 6.192254056363574e-09 | Test Loss 2.0855281421631955e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 7999 | Train Loss 6.18566983130098e-09 | Test Loss 2.011560084766618e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8099 | Train Loss 6.178919705656776e-09 | Test Loss 1.9507983678449822e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8199 | Train Loss 6.173245998184181e-09 | Test Loss 1.9010449486504762e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8299 | Train Loss 6.1670482780161925e-09 | Test Loss 1.859846768201169e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8399 | Train Loss 6.160798289940302e-09 | Test Loss 1.8271797701151667e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8499 | Train Loss 6.157122048314511e-09 | Test Loss 1.801723328249622e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8599 | Train Loss 6.152924355124376e-09 | Test Loss 1.7802240570985687e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8699 | Train Loss 6.149142058241614e-09 | Test Loss 1.7613252214743738e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8799 | Train Loss 6.14570700568926e-09 | Test Loss 1.7450372371487518e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8899 | Train Loss 6.142379676017166e-09 | Test Loss 1.73065925487625e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 8999 | Train Loss 6.139209017589496e-09 | Test Loss 1.718047160610607e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9099 | Train Loss 6.135940385698628e-09 | Test Loss 1.7068227460832028e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9199 | Train Loss 6.133646499569271e-09 | Test Loss 1.6977058072678824e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9299 | Train Loss 6.1309421197958145e-09 | Test Loss 1.689309752269621e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9399 | Train Loss 6.1288218909390116e-09 | Test Loss 1.681631832486188e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9499 | Train Loss 6.126753775985442e-09 | Test Loss 1.6746220092562444e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9599 | Train Loss 6.124825141124382e-09 | Test Loss 1.667997015817979e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9699 | Train Loss 6.123013023510173e-09 | Test Loss 1.6617445899310678e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9799 | Train Loss 6.121347702325963e-09 | Test Loss 1.6558026676912925e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9899 | Train Loss 6.119636413801683e-09 | Test Loss 1.6500492857778818e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 9999 | Train Loss 6.118159221561167e-09 | Test Loss 1.6448891062653378e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10099 | Train Loss 6.116613742731619e-09 | Test Loss 1.640344858320181e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10199 | Train Loss 6.11518394721227e-09 | Test Loss 1.6362587319848624e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10299 | Train Loss 6.113948228609706e-09 | Test Loss 1.6323403506841112e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10399 | Train Loss 6.112722412180965e-09 | Test Loss 1.6283754361147098e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10499 | Train Loss 6.111466481228383e-09 | Test Loss 1.6243444007248853e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10599 | Train Loss 6.11027450808829e-09 | Test Loss 1.6204669564992395e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10699 | Train Loss 6.1091309368055675e-09 | Test Loss 1.6166745714676472e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10799 | Train Loss 6.108046315568245e-09 | Test Loss 1.61323694378317e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10899 | Train Loss 6.1070030544457945e-09 | Test Loss 1.6099049729093436e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 10999 | Train Loss 6.105932909940849e-09 | Test Loss 1.606559503969319e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11099 | Train Loss 6.10498078977662e-09 | Test Loss 1.6031626726183842e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11199 | Train Loss 6.1040949229165385e-09 | Test Loss 1.5998437247797403e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11299 | Train Loss 6.103188216802245e-09 | Test Loss 1.5965299564197592e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11399 | Train Loss 6.102257967180062e-09 | Test Loss 1.5933487015988608e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11499 | Train Loss 6.101398831878609e-09 | Test Loss 1.5902726309732555e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11599 | Train Loss 6.100478076906145e-09 | Test Loss 1.5872778437123176e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11699 | Train Loss 6.099692496967551e-09 | Test Loss 1.584331583971371e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11799 | Train Loss 6.0988615010423884e-09 | Test Loss 1.581362824811463e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11899 | Train Loss 6.0981686488360545e-09 | Test Loss 1.578488915800081e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 11999 | Train Loss 6.097329463426888e-09 | Test Loss 1.5757169543152804e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12099 | Train Loss 6.096496075627351e-09 | Test Loss 1.5730270854357115e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12199 | Train Loss 6.09565672651896e-09 | Test Loss 1.57030602595638e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12299 | Train Loss 6.094958775993002e-09 | Test Loss 1.5676259644961956e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12399 | Train Loss 6.094248227773615e-09 | Test Loss 1.5649661346780833e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12499 | Train Loss 6.09345168799532e-09 | Test Loss 1.5623315757309333e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12599 | Train Loss 6.092804733397003e-09 | Test Loss 1.55973266526232e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12699 | Train Loss 6.09213232599072e-09 | Test Loss 1.5571387724375412e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12799 | Train Loss 6.0915687446595175e-09 | Test Loss 1.5545548213894593e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12899 | Train Loss 6.090879729188501e-09 | Test Loss 1.551996047266214e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 12999 | Train Loss 6.090264904709074e-09 | Test Loss 1.5494771655294477e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13099 | Train Loss 6.089577067021278e-09 | Test Loss 1.5469340683778937e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13199 | Train Loss 6.088922875027448e-09 | Test Loss 1.5444519853228205e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13299 | Train Loss 6.088332294812272e-09 | Test Loss 1.5420155770543746e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13399 | Train Loss 6.087697646615817e-09 | Test Loss 1.53961068361026e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13499 | Train Loss 6.0870910469873845e-09 | Test Loss 1.5372432156605452e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13599 | Train Loss 6.086456807041519e-09 | Test Loss 1.534689094596758e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13699 | Train Loss 6.085879835958614e-09 | Test Loss 1.532177012603283e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13799 | Train Loss 6.085260134970264e-09 | Test Loss 1.529800214798977e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13899 | Train Loss 6.084660822373337e-09 | Test Loss 1.527439373416856e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 13999 | Train Loss 6.0839869492849884e-09 | Test Loss 1.5250885635333826e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14099 | Train Loss 6.083370204859834e-09 | Test Loss 1.5228109962372495e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14199 | Train Loss 6.0828446224505535e-09 | Test Loss 1.5206318142219554e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14299 | Train Loss 6.0823654279932066e-09 | Test Loss 1.5185190951488145e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14399 | Train Loss 6.0817688918004665e-09 | Test Loss 1.5164183168644824e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14499 | Train Loss 6.081272225309247e-09 | Test Loss 1.514282448413658e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14599 | Train Loss 6.080652078781175e-09 | Test Loss 1.5121321266898093e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14699 | Train Loss 6.080098597691333e-09 | Test Loss 1.510020770491578e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14799 | Train Loss 6.079658392762312e-09 | Test Loss 1.5079448170721882e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14899 | Train Loss 6.079071780780304e-09 | Test Loss 1.5057934190149358e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 14999 | Train Loss 6.078474053006854e-09 | Test Loss 1.503724553486157e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15099 | Train Loss 6.077989422449323e-09 | Test Loss 1.501867035015773e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15199 | Train Loss 6.077447959853063e-09 | Test Loss 1.4999461923941533e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15299 | Train Loss 6.076943714307426e-09 | Test Loss 1.498094415262059e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15399 | Train Loss 6.0764630439863155e-09 | Test Loss 1.496289505189635e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15499 | Train Loss 6.075958711505948e-09 | Test Loss 1.494554605512283e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15599 | Train Loss 6.075330682992832e-09 | Test Loss 1.4927945029223572e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15699 | Train Loss 6.0747341981355195e-09 | Test Loss 1.4909958722357436e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15799 | Train Loss 6.074360806831299e-09 | Test Loss 1.4891417869137216e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15899 | Train Loss 6.073925284691232e-09 | Test Loss 1.4873367976211879e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 15999 | Train Loss 6.073281116658113e-09 | Test Loss 1.4854574625262064e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16099 | Train Loss 6.072865084562076e-09 | Test Loss 1.48360127450847e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16199 | Train Loss 6.072315214548654e-09 | Test Loss 1.4815909844298043e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16299 | Train Loss 6.0718812689965005e-09 | Test Loss 1.4795356986393848e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16399 | Train Loss 6.07127282424856e-09 | Test Loss 1.4772412691776633e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16499 | Train Loss 6.07089724277447e-09 | Test Loss 1.4750396105057402e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16599 | Train Loss 6.0703706346435605e-09 | Test Loss 1.4728793113770565e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16699 | Train Loss 6.069783431094167e-09 | Test Loss 1.470992820012868e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16799 | Train Loss 6.069240910573493e-09 | Test Loss 1.4691481294445372e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16899 | Train Loss 6.068801425773713e-09 | Test Loss 1.4674471077766848e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 16999 | Train Loss 6.068310835605409e-09 | Test Loss 1.4657347427265135e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17099 | Train Loss 6.06796134738751e-09 | Test Loss 1.464065373564456e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17199 | Train Loss 6.06748512933283e-09 | Test Loss 1.4624032778798446e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17299 | Train Loss 6.066999870650979e-09 | Test Loss 1.460742750055929e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17399 | Train Loss 6.066545587556897e-09 | Test Loss 1.459112097006286e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17499 | Train Loss 6.066135568340662e-09 | Test Loss 1.4573552275955965e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17599 | Train Loss 6.0656047980345156e-09 | Test Loss 1.4554596987281877e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17699 | Train Loss 6.065179445887741e-09 | Test Loss 1.4532934122762656e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17799 | Train Loss 6.064886562824007e-09 | Test Loss 1.4511555286716747e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17899 | Train Loss 6.0645074346859845e-09 | Test Loss 1.4490384713956634e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 17999 | Train Loss 6.064032555436336e-09 | Test Loss 1.4469902034761958e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18099 | Train Loss 6.063552699918582e-09 | Test Loss 1.4450471700770656e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18199 | Train Loss 6.063184830421027e-09 | Test Loss 1.4432314526634199e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18299 | Train Loss 6.062791114884699e-09 | Test Loss 1.4415157846650567e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18399 | Train Loss 6.062282146577961e-09 | Test Loss 1.4398619077499678e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18499 | Train Loss 6.061790509132203e-09 | Test Loss 1.4382362594198467e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18599 | Train Loss 6.061480839786598e-09 | Test Loss 1.43661770199265e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18699 | Train Loss 6.061061133167926e-09 | Test Loss 1.4350759501398231e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18799 | Train Loss 6.060834358607236e-09 | Test Loss 1.433540382099579e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18899 | Train Loss 6.06033655137413e-09 | Test Loss 1.4320309937196517e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 18999 | Train Loss 6.059945033519125e-09 | Test Loss 1.4305526962836359e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19099 | Train Loss 6.059594888363343e-09 | Test Loss 1.4291128282235079e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19199 | Train Loss 6.0591312730893385e-09 | Test Loss 1.427649885447339e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19299 | Train Loss 6.058752956839664e-09 | Test Loss 1.4262215711888087e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19399 | Train Loss 6.0584386272107204e-09 | Test Loss 1.4248308882657509e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19499 | Train Loss 6.058078199478032e-09 | Test Loss 1.42340971146123e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19599 | Train Loss 6.057776322991059e-09 | Test Loss 1.4220079472737619e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19699 | Train Loss 6.057395573692469e-09 | Test Loss 1.4206410722369338e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19799 | Train Loss 6.057074921965645e-09 | Test Loss 1.419305606967053e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19899 | Train Loss 6.056671841325143e-09 | Test Loss 1.41801491721827e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 19999 | Train Loss 6.056223129184171e-09 | Test Loss 1.4167461533812354e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20099 | Train Loss 6.056021642289121e-09 | Test Loss 1.4156295254711371e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20199 | Train Loss 6.055605975807519e-09 | Test Loss 1.4145001490182546e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20299 | Train Loss 6.055275051421352e-09 | Test Loss 1.4134299428040454e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20399 | Train Loss 6.0547832473813404e-09 | Test Loss 1.4123343299797128e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20499 | Train Loss 6.054406163867068e-09 | Test Loss 1.4113691902166814e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20599 | Train Loss 6.054079172609795e-09 | Test Loss 1.4103922969186288e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20699 | Train Loss 6.05375379934431e-09 | Test Loss 1.409475596691459e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20799 | Train Loss 6.053348051019493e-09 | Test Loss 1.4085177136934273e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20899 | Train Loss 6.053038012630125e-09 | Test Loss 1.4076817088736752e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 20999 | Train Loss 6.052739538486347e-09 | Test Loss 1.4068322558855994e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21099 | Train Loss 6.052212004854921e-09 | Test Loss 1.4059642877712825e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21199 | Train Loss 6.0519079437606865e-09 | Test Loss 1.405211538717621e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21299 | Train Loss 6.051584357505208e-09 | Test Loss 1.4044908115160748e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21399 | Train Loss 6.0511160339949606e-09 | Test Loss 1.4036840525204253e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21499 | Train Loss 6.05083781844927e-09 | Test Loss 1.4029824057847593e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21599 | Train Loss 6.05048353628004e-09 | Test Loss 1.4022019883945258e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21699 | Train Loss 6.0502116048118106e-09 | Test Loss 1.4014815390704792e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21799 | Train Loss 6.049857910556103e-09 | Test Loss 1.400676711278905e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21899 | Train Loss 6.049504372264218e-09 | Test Loss 1.3997315271358417e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 21999 | Train Loss 6.049302544170293e-09 | Test Loss 1.3989392892157926e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22099 | Train Loss 6.048881144485145e-09 | Test Loss 1.398017318055597e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22199 | Train Loss 6.048511483368324e-09 | Test Loss 1.3969559202503062e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22299 | Train Loss 6.048198434417548e-09 | Test Loss 1.3960676761862518e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22399 | Train Loss 6.047697493115621e-09 | Test Loss 1.395218661467588e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22499 | Train Loss 6.047590023092375e-09 | Test Loss 1.394283679439265e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22599 | Train Loss 6.047229137197747e-09 | Test Loss 1.3933169220043618e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22699 | Train Loss 6.04678089311609e-09 | Test Loss 1.3925067429661023e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22799 | Train Loss 6.046596791784363e-09 | Test Loss 1.3915682199933337e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22899 | Train Loss 6.046276276612918e-09 | Test Loss 1.3907794877077284e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 22999 | Train Loss 6.045948402979997e-09 | Test Loss 1.3898190499309667e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23099 | Train Loss 6.045816667596126e-09 | Test Loss 1.3890545811453908e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23199 | Train Loss 6.045548384739069e-09 | Test Loss 1.3882582157072666e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23299 | Train Loss 6.045228447786764e-09 | Test Loss 1.38744297973823e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23399 | Train Loss 6.0449628757080954e-09 | Test Loss 1.3866612307865004e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23499 | Train Loss 6.0447594843304835e-09 | Test Loss 1.3858699106994957e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23599 | Train Loss 6.044339027786108e-09 | Test Loss 1.3850549785506254e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23699 | Train Loss 6.044234924265252e-09 | Test Loss 1.3842780046346031e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23799 | Train Loss 6.043965615681939e-09 | Test Loss 1.3835100232559521e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23899 | Train Loss 6.04372520170191e-09 | Test Loss 1.3827354440052483e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 23999 | Train Loss 6.043328055706201e-09 | Test Loss 1.3817977606066612e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24099 | Train Loss 6.043102658930187e-09 | Test Loss 1.3809090777336115e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24199 | Train Loss 6.042885581670667e-09 | Test Loss 1.380133882560478e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24299 | Train Loss 6.0426771820685875e-09 | Test Loss 1.3793102429645722e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24399 | Train Loss 6.042439490255827e-09 | Test Loss 1.3784300647379138e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24499 | Train Loss 6.042152247385344e-09 | Test Loss 1.3775827711425409e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24599 | Train Loss 6.041882008012341e-09 | Test Loss 1.3767554755062836e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24699 | Train Loss 6.041598286626018e-09 | Test Loss 1.3759202255765045e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24799 | Train Loss 6.041288267605541e-09 | Test Loss 1.3751127660709395e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24899 | Train Loss 6.041137607576214e-09 | Test Loss 1.3742712862473763e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 24999 | Train Loss 6.040921756040609e-09 | Test Loss 1.3734729308784337e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25099 | Train Loss 6.040571682290466e-09 | Test Loss 1.3727090210620369e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25199 | Train Loss 6.040481731513957e-09 | Test Loss 1.3719684204157583e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25299 | Train Loss 6.040067662909809e-09 | Test Loss 1.3711041289296542e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25399 | Train Loss 6.039929940326106e-09 | Test Loss 1.3703630813633585e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25499 | Train Loss 6.039758399646797e-09 | Test Loss 1.369641426108995e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25599 | Train Loss 6.0395138946651395e-09 | Test Loss 1.3690033933543024e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25699 | Train Loss 6.039208223791336e-09 | Test Loss 1.3682727544932221e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25799 | Train Loss 6.0391292381710775e-09 | Test Loss 1.367708471938221e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25899 | Train Loss 6.038826203984962e-09 | Test Loss 1.3670896013461665e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 25999 | Train Loss 6.038461952793011e-09 | Test Loss 1.3663966714808785e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26099 | Train Loss 6.0383917307943584e-09 | Test Loss 1.365847950700805e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26199 | Train Loss 6.038192682729372e-09 | Test Loss 1.3652182235717607e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26299 | Train Loss 6.037975007383046e-09 | Test Loss 1.3646006347381207e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26399 | Train Loss 6.0375635221963154e-09 | Test Loss 1.3639992400627742e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26499 | Train Loss 6.0375107978976704e-09 | Test Loss 1.3633541133633229e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26599 | Train Loss 6.037284441737683e-09 | Test Loss 1.3627324052365702e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26699 | Train Loss 6.0371642940903545e-09 | Test Loss 1.3620605748857484e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26799 | Train Loss 6.0368201322548875e-09 | Test Loss 1.3612366576801707e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26899 | Train Loss 6.036647002637245e-09 | Test Loss 1.3604588640243471e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 26999 | Train Loss 6.036468051180064e-09 | Test Loss 1.3596546521127073e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27099 | Train Loss 6.036197483932727e-09 | Test Loss 1.3589048124857602e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27199 | Train Loss 6.036120328183692e-09 | Test Loss 1.3582951558195056e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27299 | Train Loss 6.035807746799483e-09 | Test Loss 1.3576476306143187e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27399 | Train Loss 6.035644504333244e-09 | Test Loss 1.3570453467532632e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27499 | Train Loss 6.035433498784123e-09 | Test Loss 1.3564603001595473e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27599 | Train Loss 6.035282062459271e-09 | Test Loss 1.3558755576151593e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27699 | Train Loss 6.035216995912151e-09 | Test Loss 1.3553108071172217e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27799 | Train Loss 6.034908885924963e-09 | Test Loss 1.354751444976369e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27899 | Train Loss 6.034648175317087e-09 | Test Loss 1.3541332223279235e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 27999 | Train Loss 6.034643115782971e-09 | Test Loss 1.3535806311204447e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28099 | Train Loss 6.034307300583266e-09 | Test Loss 1.3530291652481748e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28199 | Train Loss 6.034036777156143e-09 | Test Loss 1.3525368594572734e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28299 | Train Loss 6.0339599967332554e-09 | Test Loss 1.3520155403455824e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28399 | Train Loss 6.0337092633602205e-09 | Test Loss 1.3515150909976954e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28499 | Train Loss 6.033706266628656e-09 | Test Loss 1.3510409024722871e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28599 | Train Loss 6.033430990422919e-09 | Test Loss 1.3505138346348325e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28699 | Train Loss 6.0333771299938226e-09 | Test Loss 1.3499846666499037e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28799 | Train Loss 6.033144317344087e-09 | Test Loss 1.349528483443887e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28899 | Train Loss 6.032937639852587e-09 | Test Loss 1.3488828390456529e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 28999 | Train Loss 6.032658723778744e-09 | Test Loss 1.3482874112008903e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29099 | Train Loss 6.032654612463066e-09 | Test Loss 1.3477286058432476e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29199 | Train Loss 6.0322046368156565e-09 | Test Loss 1.347048069758323e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29299 | Train Loss 6.032211320960937e-09 | Test Loss 1.3462442139629802e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29399 | Train Loss 6.032120193577636e-09 | Test Loss 1.3458426016213938e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29499 | Train Loss 6.031813001072284e-09 | Test Loss 1.345209680638555e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29599 | Train Loss 6.031787888021743e-09 | Test Loss 1.3444565099955246e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29699 | Train Loss 6.031711055788674e-09 | Test Loss 1.3439537030959277e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29799 | Train Loss 6.031349061612333e-09 | Test Loss 1.3430242312340557e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29899 | Train Loss 6.0311626223877335e-09 | Test Loss 1.3422934785983052e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n",
            "Epoch 29999 | Train Loss 6.031079216222169e-09 | Test Loss 1.3416076094578422e-08 | Train Acc 0.9999999403953552 | Test Acc 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subtitle = f\"(frac_train={frac_train}, lr={lr}, wd={wd})\"\n",
        "\n",
        "fig = line(\n",
        "    [train_losses[::100], test_losses[::100]],\n",
        "    x=np.arange(0, len(train_losses), 100),\n",
        "    xaxis=\"Epoch\",\n",
        "    yaxis=\"Loss\",\n",
        "    log_y=True,\n",
        "    title=f\"Training Curve for XOR {subtitle}\", # add the hyper-params right in the title\n",
        "    line_labels=[\"train\", \"test\"],\n",
        "    toggle_x=True,\n",
        "    toggle_y=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2P_ZHK1GsHfm",
        "outputId": "7d0d6142-cb2d-42d0-a4b4-94cb4164cc87"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"355df3ab-0ce5-4e60-b84f-c6299bc5e47e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"355df3ab-0ce5-4e60-b84f-c6299bc5e47e\")) {                    Plotly.newPlot(                        \"355df3ab-0ce5-4e60-b84f-c6299bc5e47e\",                        [{\"hovertemplate\":\"Color=train\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,20100,20200,20300,20400,20500,20600,20700,20800,20900,21000,21100,21200,21300,21400,21500,21600,21700,21800,21900,22000,22100,22200,22300,22400,22500,22600,22700,22800,22900,23000,23100,23200,23300,23400,23500,23600,23700,23800,23900,24000,24100,24200,24300,24400,24500,24600,24700,24800,24900,25000,25100,25200,25300,25400,25500,25600,25700,25800,25900,26000,26100,26200,26300,26400,26500,26600,26700,26800,26900,27000,27100,27200,27300,27400,27500,27600,27700,27800,27900,28000,28100,28200,28300,28400,28500,28600,28700,28800,28900,29000,29100,29200,29300,29400,29500,29600,29700,29800,29900],\"xaxis\":\"x\",\"y\":[0.6952897690341573,0.17025224390431226,0.00054712795814494,0.00018680550746672753,0.00005584721842772638,0.00001811251550392042,6.038455661465693e-6,2.055470731887819e-6,7.264860952468446e-7,2.6769810355267094e-7,1.055895919408301e-7,4.5183657515540366e-8,2.1388716284643873e-8,1.1778368716471042e-8,7.839011372946537e-9,6.361359718851845e-9,6.059596605752227e-9,6.225387747757409e-9,6.505534659312981e-9,6.770722327583349e-9,6.974984186697193e-9,7.132516377524177e-9,7.242817013022228e-9,7.322391820106789e-9,7.37479468887232e-9,7.405513398798046e-9,7.423688674161653e-9,7.429629323989947e-9,7.427938644724433e-9,7.42167762924988e-9,7.411381745450559e-9,7.39966039745435e-9,7.3858551872861414e-9,7.3713246229175074e-9,7.356583180421221e-9,7.34246385824082e-9,7.327436926351242e-9,7.312497163644003e-9,7.2986338731925475e-9,7.283566547079067e-9,7.269635550910165e-9,7.2562281821936086e-9,7.240792469038845e-9,7.226066110562512e-9,7.207885560687172e-9,7.188338350638757e-9,7.167627349038939e-9,7.14295107259549e-9,7.114146373196578e-9,7.077055306389841e-9,7.0372368612930575e-9,6.984748001075771e-9,6.931444347189878e-9,6.878627539859549e-9,6.823654638952881e-9,6.775705000261446e-9,6.725801909683783e-9,6.679958249986828e-9,6.641183481142085e-9,6.605109212647939e-9,6.570561798246751e-9,6.5344208256173775e-9,6.50399906711545e-9,6.468561866311343e-9,6.43957745224515e-9,6.410453895418644e-9,6.384570634790479e-9,6.363238462254729e-9,6.341171692141635e-9,6.321330090382588e-9,6.301449066555745e-9,6.284264142674381e-9,6.269260468158149e-9,6.255250993050207e-9,6.2425955209103005e-9,6.231507700890527e-9,6.220223649986832e-9,6.210195459105878e-9,6.200516515890862e-9,6.192209218129737e-9,6.185601994375824e-9,6.178857767806637e-9,6.173178377248369e-9,6.166994822897271e-9,6.160745171640585e-9,6.157078940667077e-9,6.152890930511148e-9,6.1491040493872215e-9,6.14565846606068e-9,6.142337203267373e-9,6.139174931684409e-9,6.13592536584705e-9,6.133631851647252e-9,6.130917279371238e-9,6.128799386948286e-9,6.126720368078379e-9,6.124820697086785e-9,6.123000254615214e-9,6.121348061182408e-9,6.1196160767430466e-9,6.118128719849247e-9,6.116613422378187e-9,6.115159123251262e-9,6.113913052576551e-9,6.112675459066215e-9,6.111442117828458e-9,6.1102323859562875e-9,6.109079131399713e-9,6.108045035363559e-9,6.106981990720231e-9,6.105921626347394e-9,6.10497789036309e-9,6.104100314706628e-9,6.10316935415254e-9,6.102195813088719e-9,6.101393480050816e-9,6.100478551021324e-9,6.099674600466233e-9,6.0988738239158165e-9,6.098178333325401e-9,6.097290779720427e-9,6.0965016666946e-9,6.095687414683303e-9,6.094960817258035e-9,6.094221836186711e-9,6.09346673957379e-9,6.092799109397021e-9,6.092164769687424e-9,6.091567620150169e-9,6.090860906488167e-9,6.090249440279671e-9,6.0895446376119255e-9,6.08892872373525e-9,6.0883263884735826e-9,6.087706500795305e-9,6.087095084226074e-9,6.086448185319413e-9,6.0858614452425935e-9,6.0852251855872e-9,6.084639421826842e-9,6.083980532755983e-9,6.083361317746634e-9,6.082820089550152e-9,6.0823384850474984e-9,6.08176578099628e-9,6.081272315387177e-9,6.080652332547122e-9,6.080125752995011e-9,6.0796284404727665e-9,6.07904766048887e-9,6.0784508482559655e-9,6.07802855072712e-9,6.077434441284121e-9,6.0769771532079484e-9,6.076472078806214e-9,6.075972304413153e-9,6.075338493540352e-9,6.074824515566728e-9,6.074369530981943e-9,6.073907548969276e-9,6.07329483402832e-9,6.072868388113553e-9,6.072289504348408e-9,6.07186529970483e-9,6.071209380267777e-9,6.070856318039107e-9,6.070397978340341e-9,6.069748951464709e-9,6.069249595255748e-9,6.068799743126741e-9,6.068262395735554e-9,6.067972457848099e-9,6.067467616867956e-9,6.066997600567376e-9,6.066541816190922e-9,6.066083784247521e-9,6.065606380191762e-9,6.065206154195238e-9,6.064872186830557e-9,6.064499722452609e-9,6.063990739136481e-9,6.063511500113179e-9,6.06318639248172e-9,6.062771467689707e-9,6.062288778619317e-9,6.061800973075004e-9,6.061481494539655e-9,6.061092971268083e-9,6.060799485738815e-9,6.060257710527549e-9,6.0599389356460016e-9,6.059645702920908e-9,6.05911733149697e-9,6.0588109584266676e-9,6.058413218238017e-9,6.058092585643558e-9,6.057790365558126e-9,6.057373452286001e-9,6.05709600191851e-9,6.056666979107225e-9,6.056236709501741e-9,6.056055461111415e-9,6.055568619041639e-9,6.055229383117836e-9,6.054780885766743e-9,6.054406441607437e-9,6.054120011386311e-9,6.053741217385559e-9,6.053313231902451e-9,6.053073107786866e-9,6.0526604869675004e-9,6.052266274786324e-9,6.051907010541376e-9,6.051593840772454e-9,6.051158673821571e-9,6.0508592042256866e-9,6.050557863349615e-9,6.050167105810289e-9,6.04984314955491e-9,6.049431348157468e-9,6.049288585627112e-9,6.048820669890688e-9,6.0485420550600265e-9,6.04819538318239e-9,6.047705686973897e-9,6.0475359781136695e-9,6.047234311545233e-9,6.046787624919283e-9,6.046658185793425e-9,6.046288355404715e-9,6.045970818828765e-9,6.045832885812117e-9,6.0454856122190514e-9,6.045246948680174e-9,6.044977103516106e-9,6.044784634211238e-9,6.0443108796910096e-9,6.044254045274633e-9,6.043933905678867e-9,6.043702579304837e-9,6.043299250926765e-9,6.0430921097465926e-9,6.042845248245166e-9,6.0426617391890915e-9,6.042428533803389e-9,6.042093170931602e-9,6.041865452973112e-9,6.041519079416473e-9,6.041218192548026e-9,6.041161668778814e-9,6.04093367041732e-9,6.0406073859584775e-9,6.040500818391585e-9,6.040095332768334e-9,6.039921937272587e-9,6.039739992706617e-9,6.039535798633093e-9,6.039227737560134e-9,6.039064023403438e-9,6.038880882890385e-9,6.038491480612867e-9,6.038382399597817e-9,6.038220933492152e-9,6.037927414529844e-9,6.037603792663303e-9,6.037517940609974e-9,6.037271498982305e-9,6.037168528678481e-9,6.036839549657307e-9,6.036660665041938e-9,6.036497208035293e-9,6.036219080857187e-9,6.036111417826983e-9,6.035814413466435e-9,6.03558722408686e-9,6.035455417035243e-9,6.035280108613115e-9,6.035164183433401e-9,6.034883812317089e-9,6.034713673148486e-9,6.034631391486567e-9,6.034358221203845e-9,6.034052706503825e-9,6.033963614097465e-9,6.033721054004222e-9,6.0336891672587296e-9,6.033460540038668e-9,6.0333967604826325e-9,6.033099591451262e-9,6.032917705232147e-9,6.032665498923394e-9,6.0326804824819475e-9,6.032249108211424e-9,6.0321084157560044e-9,6.032078447977355e-9,6.0318421620432766e-9,6.0317908528212204e-9,6.031691910059694e-9,6.031345418344401e-9,6.031151502730456e-9],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Color=test\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,20100,20200,20300,20400,20500,20600,20700,20800,20900,21000,21100,21200,21300,21400,21500,21600,21700,21800,21900,22000,22100,22200,22300,22400,22500,22600,22700,22800,22900,23000,23100,23200,23300,23400,23500,23600,23700,23800,23900,24000,24100,24200,24300,24400,24500,24600,24700,24800,24900,25000,25100,25200,25300,25400,25500,25600,25700,25800,25900,26000,26100,26200,26300,26400,26500,26600,26700,26800,26900,27000,27100,27200,27300,27400,27500,27600,27700,27800,27900,28000,28100,28200,28300,28400,28500,28600,28700,28800,28900,29000,29100,29200,29300,29400,29500,29600,29700,29800,29900],\"xaxis\":\"x\",\"y\":[0.6941351577511856,0.22514343385659913,0.00937259900414802,0.008008361257795998,0.007129971948923736,0.006034666763267428,0.0049424655190024175,0.003951191097735521,0.003097959715763688,0.0023366059872867574,0.0016838283186530205,0.0015036325571536125,0.0034521373730951703,0.009012868780428101,0.012124990366078026,0.014774784890541534,0.016809431006739824,0.01825365679669069,0.019154798008255727,0.019003857380884427,0.01831885963750522,0.01752790845334718,0.01668569701080335,0.015857068222872694,0.015061443837281464,0.014285617776154476,0.013533045785339108,0.012812719081987478,0.012125867326390217,0.011474720651270913,0.01083844257443742,0.010212267465469398,0.009594072967825428,0.008988723414937774,0.008390265414424294,0.007797751814501656,0.007202915895540138,0.006597320741705142,0.0059866518731817725,0.005358351048766289,0.004706715642281337,0.00403622422799266,0.0033666846421002206,0.0027184678912717474,0.0021074982999153527,0.001561029480114455,0.0011333349946252883,0.000836343180204994,0.0006527882926313569,0.0005232875874972493,0.00039420747925133555,0.00024960867728141196,0.00012189809255286854,0.00004267829581800212,0.000010537616203437294,2.2963691284966527e-6,8.592253048806227e-7,8.141966218710879e-7,1.0329592934019128e-6,1.193855453053517e-6,1.0664286291303874e-6,7.594797286743012e-7,4.932082837098082e-7,3.1920032320411806e-7,2.1500039605916553e-7,1.5138452583023906e-7,1.1120226941521665e-7,8.531911996888664e-8,6.795167032988371e-8,5.571005924861639e-8,4.6831316109905826e-8,4.022329101613556e-8,3.528171680815894e-8,3.1540816480406984e-8,2.861978598976605e-8,2.6278253084095784e-8,2.441429429752271e-8,2.2939506787589754e-8,2.178666637738647e-8,2.0847069322038228e-8,2.0108638938951496e-8,1.9502306437817847e-8,1.900591211920942e-8,1.8594848595357677e-8,1.8269226716545545e-8,1.8014985170790103e-8,1.7800227612886108e-8,1.7611630941492206e-8,1.7448840534091617e-8,1.7305241309537744e-8,1.717938365915634e-8,1.7067145241464132e-8,1.6976168637234966e-8,1.6892410605501286e-8,1.681557448892288e-8,1.67455451860684e-8,1.66792991415538e-8,1.6616804432031854e-8,1.655747481164862e-8,1.6500111762788216e-8,1.6448518312264823e-8,1.6403163682293033e-8,1.6362078839341795e-8,1.6322988268403776e-8,1.6283280613326175e-8,1.624344867409745e-8,1.6204173833128736e-8,1.616674119757634e-8,1.613226509478211e-8,1.609886301528802e-8,1.60651978128809e-8,1.603157490895052e-8,1.5997977243861475e-8,1.596516261801305e-8,1.593311677050881e-8,1.590260173586487e-8,1.587235297652567e-8,1.5842749961128005e-8,1.581320515020339e-8,1.5784582442527964e-8,1.5756894482543638e-8,1.573001338780889e-8,1.5702958159039053e-8,1.567591634980735e-8,1.564959646456457e-8,1.5623124567864358e-8,1.5597184730962106e-8,1.5571140143251677e-8,1.554532552054959e-8,1.5519601275245875e-8,1.549456828298219e-8,1.5468988971629655e-8,1.54442520314879e-8,1.5419832716301056e-8,1.539590950604155e-8,1.5372311077329287e-8,1.534674914495588e-8,1.532164250205313e-8,1.5297665347476256e-8,1.527411164942371e-8,1.52505953600084e-8,1.5227835025589566e-8,1.5206084325377864e-8,1.518494971452085e-8,1.516386413260139e-8,1.514250254615817e-8,1.5120756191789616e-8,1.510005378134452e-8,1.5079246141983674e-8,1.50578430450918e-8,1.503718746102276e-8,1.501851790594025e-8,1.499992836401931e-8,1.4980940821994833e-8,1.496261323102933e-8,1.494518661431976e-8,1.4927660102923202e-8,1.4909890722348987e-8,1.4891195607503854e-8,1.4872827401037809e-8,1.4854320571406067e-8,1.4836010304581219e-8,1.4815822167986939e-8,1.4794666416189444e-8,1.47720026983031e-8,1.4749979433171799e-8,1.4728551498398029e-8,1.4709429921251222e-8,1.46910372782323e-8,1.467435033540652e-8,1.4657071049430655e-8,1.464031432606353e-8,1.4624137457111757e-8,1.4607291972265987e-8,1.4590721736647394e-8,1.4573139577709154e-8,1.455485896002408e-8,1.4533062180191696e-8,1.4511745225798247e-8,1.44903838749249e-8,1.4470045735264924e-8,1.4450086507091416e-8,1.4431810011191453e-8,1.4414639495846517e-8,1.439819329534699e-8,1.4382063273472987e-8,1.4366147435593643e-8,1.4350569169838803e-8,1.4335550406580282e-8,1.4319978961752633e-8,1.4305013307398407e-8,1.4290577575960957e-8,1.427629331293101e-8,1.4262089131079951e-8,1.4248075791580865e-8,1.4234115714698203e-8,1.4219985796856909e-8,1.4206249450419891e-8,1.4193058925460969e-8,1.4179806021328987e-8,1.4167550936967485e-8,1.4155870997313827e-8,1.4144491034214379e-8,1.4134512472497512e-8,1.4123347382891819e-8,1.4113578141231876e-8,1.410352219105059e-8,1.4094463479929145e-8,1.4085242691545124e-8,1.4076406204492074e-8,1.4068055675924713e-8,1.405993734852347e-8,1.4052497110958479e-8,1.4044671349501278e-8,1.4037159222910507e-8,1.4030137416741429e-8,1.4022307651735777e-8,1.4015167941475703e-8,1.4007016780318016e-8,1.399719057378409e-8,1.3989436790842605e-8,1.3980326272943102e-8,1.3969678944926769e-8,1.3961047638310895e-8,1.395253462502521e-8,1.394263161482492e-8,1.3933093021352312e-8,1.3924907684884321e-8,1.391528906985674e-8,1.390728808322718e-8,1.389785138259029e-8,1.3890631893142536e-8,1.3881992994619532e-8,1.3874507713874896e-8,1.3866290991205158e-8,1.3858927428423365e-8,1.3850652407777656e-8,1.384279813531609e-8,1.3834574866364189e-8,1.382698298357449e-8,1.3818299599042057e-8,1.3809262231330444e-8,1.3801436383770835e-8,1.379293967033529e-8,1.3784235718476467e-8,1.3775698337144273e-8,1.3767586489451118e-8,1.3758951573725184e-8,1.375065813447358e-8,1.3742877816147146e-8,1.3734687189521478e-8,1.3726568582049652e-8,1.3719579763491935e-8,1.3710656048130529e-8,1.3702925478189904e-8,1.3696789470204509e-8,1.3689848089406894e-8,1.3683166118873281e-8,1.3677042936529379e-8,1.3670530369870259e-8,1.3664355315950737e-8,1.365861689063294e-8,1.365209650026348e-8,1.3646320347760644e-8,1.3639920763643304e-8,1.3633670352940968e-8,1.36271445497533e-8,1.3620249837475705e-8,1.3612148134642321e-8,1.3604473909422581e-8,1.3596478101297411e-8,1.3589148343157402e-8,1.3582972927875751e-8,1.3576343225358797e-8,1.3570209843930936e-8,1.3564721990382525e-8,1.3559153726155648e-8,1.3553557654291055e-8,1.3547123829655896e-8,1.354166272071611e-8,1.3536316916596004e-8,1.3529887907826194e-8,1.3525269384834517e-8,1.3520255371711442e-8,1.3515003870653488e-8,1.3510343387647217e-8,1.3505123245587714e-8,1.3500224040771695e-8,1.3494441171515377e-8,1.348885717741034e-8,1.3482671997353992e-8,1.3477206301139742e-8,1.3470107891335391e-8,1.3463144329062027e-8,1.3458169453121847e-8,1.3451316295801303e-8,1.344483192008262e-8,1.3438957812009257e-8,1.3430907343732114e-8,1.342316412301573e-8],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Curve for XOR (frac_train=0.008, lr=0.001, wd=1)\"},\"updatemenus\":[{\"active\":-1,\"buttons\":[{\"args\":[{\"xaxis.type\":\"log\"}],\"args2\":[{\"xaxis.type\":\"linear\"}],\"label\":\"Log x-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":1.0},{\"active\":0,\"buttons\":[{\"args\":[{\"yaxis.type\":\"log\"}],\"args2\":[{\"yaxis.type\":\"linear\"}],\"label\":\"Log y-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":0.85}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('355df3ab-0ce5-4e60-b84f-c6299bc5e47e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)"
      ],
      "metadata": {
        "id": "89dWTjbhKkJu"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}
